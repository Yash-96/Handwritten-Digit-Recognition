{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "epr8MKm-xt3D",
    "outputId": "4a30ca10-dcf0-4436-d350-e013a4996317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "tfnmEN_GTAtR",
    "outputId": "df3cda21-df35-4cea-c9f6-b851561f24e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/d1/1b9e85e991f836e9aaea18367ff628a6324af1005971dc9f57e51a2ab5a4/mlxtend-0.14.0-py2.py3-none-any.whl (1.3MB)\n",
      "\r",
      "\u001b[K    0% |▎                               | 10kB 21.3MB/s eta 0:00:01\r",
      "\u001b[K    1% |▌                               | 20kB 4.5MB/s eta 0:00:01\r",
      "\u001b[K    2% |▊                               | 30kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K    3% |█                               | 40kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K    3% |█▎                              | 51kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K    4% |█▌                              | 61kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K    5% |█▊                              | 71kB 6.7MB/s eta 0:00:01\r",
      "\u001b[K    6% |██                              | 81kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K    7% |██▎                             | 92kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K    7% |██▌                             | 102kB 6.8MB/s eta 0:00:01\r",
      "\u001b[K    8% |██▊                             | 112kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K    9% |███                             | 122kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K    10% |███▎                            | 133kB 9.1MB/s eta 0:00:01\r",
      "\u001b[K    11% |███▌                            | 143kB 16.1MB/s eta 0:00:01\r",
      "\u001b[K    11% |███▉                            | 153kB 16.0MB/s eta 0:00:01\r",
      "\u001b[K    12% |████                            | 163kB 16.0MB/s eta 0:00:01\r",
      "\u001b[K    13% |████▎                           | 174kB 16.4MB/s eta 0:00:01\r",
      "\u001b[K    14% |████▌                           | 184kB 16.6MB/s eta 0:00:01\r",
      "\u001b[K    14% |████▉                           | 194kB 16.5MB/s eta 0:00:01\r",
      "\u001b[K    15% |█████                           | 204kB 18.6MB/s eta 0:00:01\r",
      "\u001b[K    16% |█████▎                          | 215kB 18.3MB/s eta 0:00:01\r",
      "\u001b[K    17% |█████▌                          | 225kB 18.4MB/s eta 0:00:01\r",
      "\u001b[K    18% |█████▉                          | 235kB 19.1MB/s eta 0:00:01\r",
      "\u001b[K    18% |██████                          | 245kB 19.3MB/s eta 0:00:01\r",
      "\u001b[K    19% |██████▎                         | 256kB 19.1MB/s eta 0:00:01\r",
      "\u001b[K    20% |██████▌                         | 266kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K    21% |██████▉                         | 276kB 18.3MB/s eta 0:00:01\r",
      "\u001b[K    22% |███████                         | 286kB 18.3MB/s eta 0:00:01\r",
      "\u001b[K    22% |███████▎                        | 296kB 18.3MB/s eta 0:00:01\r",
      "\u001b[K    23% |███████▋                        | 307kB 41.1MB/s eta 0:00:01\r",
      "\u001b[K    24% |███████▉                        | 317kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K    25% |████████                        | 327kB 43.7MB/s eta 0:00:01\r",
      "\u001b[K    25% |████████▎                       | 337kB 44.6MB/s eta 0:00:01\r",
      "\u001b[K    26% |████████▋                       | 348kB 40.3MB/s eta 0:00:01\r",
      "\u001b[K    27% |████████▉                       | 358kB 41.8MB/s eta 0:00:01\r",
      "\u001b[K    28% |█████████                       | 368kB 48.9MB/s eta 0:00:01\r",
      "\u001b[K    29% |█████████▎                      | 378kB 48.6MB/s eta 0:00:01\r",
      "\u001b[K    29% |█████████▋                      | 389kB 48.6MB/s eta 0:00:01\r",
      "\u001b[K    30% |█████████▉                      | 399kB 49.3MB/s eta 0:00:01\r",
      "\u001b[K    31% |██████████                      | 409kB 48.9MB/s eta 0:00:01\r",
      "\u001b[K    32% |██████████▎                     | 419kB 49.4MB/s eta 0:00:01\r",
      "\u001b[K    33% |██████████▋                     | 430kB 45.4MB/s eta 0:00:01\r",
      "\u001b[K    33% |██████████▉                     | 440kB 44.4MB/s eta 0:00:01\r",
      "\u001b[K    34% |███████████                     | 450kB 44.7MB/s eta 0:00:01\r",
      "\u001b[K    35% |███████████▍                    | 460kB 42.5MB/s eta 0:00:01\r",
      "\u001b[K    36% |███████████▋                    | 471kB 41.5MB/s eta 0:00:01\r",
      "\u001b[K    37% |███████████▉                    | 481kB 41.3MB/s eta 0:00:01\r",
      "\u001b[K    37% |████████████                    | 491kB 40.0MB/s eta 0:00:01\r",
      "\u001b[K    38% |████████████▍                   | 501kB 39.5MB/s eta 0:00:01\r",
      "\u001b[K    39% |████████████▋                   | 512kB 37.3MB/s eta 0:00:01\r",
      "\u001b[K    40% |████████████▉                   | 522kB 37.6MB/s eta 0:00:01\r",
      "\u001b[K    40% |█████████████                   | 532kB 40.4MB/s eta 0:00:01\r",
      "\u001b[K    41% |█████████████▍                  | 542kB 41.0MB/s eta 0:00:01\r",
      "\u001b[K    42% |█████████████▋                  | 552kB 46.1MB/s eta 0:00:01\r",
      "\u001b[K    43% |█████████████▉                  | 563kB 48.0MB/s eta 0:00:01\r",
      "\u001b[K    44% |██████████████                  | 573kB 49.4MB/s eta 0:00:01\r",
      "\u001b[K    44% |██████████████▍                 | 583kB 50.1MB/s eta 0:00:01\r",
      "\u001b[K    45% |██████████████▋                 | 593kB 51.6MB/s eta 0:00:01\r",
      "\u001b[K    46% |██████████████▉                 | 604kB 52.8MB/s eta 0:00:01\r",
      "\u001b[K    47% |███████████████▏                | 614kB 57.3MB/s eta 0:00:01\r",
      "\u001b[K    48% |███████████████▍                | 624kB 55.8MB/s eta 0:00:01\r",
      "\u001b[K    48% |███████████████▋                | 634kB 56.7MB/s eta 0:00:01\r",
      "\u001b[K    49% |███████████████▉                | 645kB 56.1MB/s eta 0:00:01\r",
      "\u001b[K    50% |████████████████▏               | 655kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K    51% |████████████████▍               | 665kB 41.2MB/s eta 0:00:01\r",
      "\u001b[K    51% |████████████████▋               | 675kB 41.2MB/s eta 0:00:01\r",
      "\u001b[K    52% |████████████████▉               | 686kB 41.4MB/s eta 0:00:01\r",
      "\u001b[K    53% |█████████████████▏              | 696kB 41.5MB/s eta 0:00:01\r",
      "\u001b[K    54% |█████████████████▍              | 706kB 40.1MB/s eta 0:00:01\r",
      "\u001b[K    55% |█████████████████▋              | 716kB 40.1MB/s eta 0:00:01\r",
      "\u001b[K    55% |█████████████████▉              | 727kB 40.4MB/s eta 0:00:01\r",
      "\u001b[K    56% |██████████████████▏             | 737kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K    57% |██████████████████▍             | 747kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K    58% |██████████████████▋             | 757kB 38.9MB/s eta 0:00:01\r",
      "\u001b[K    59% |███████████████████             | 768kB 51.3MB/s eta 0:00:01\r",
      "\u001b[K    59% |███████████████████▏            | 778kB 50.7MB/s eta 0:00:01\r",
      "\u001b[K    60% |███████████████████▍            | 788kB 50.0MB/s eta 0:00:01\r",
      "\u001b[K    61% |███████████████████▋            | 798kB 50.3MB/s eta 0:00:01\r",
      "\u001b[K    62% |████████████████████            | 808kB 52.2MB/s eta 0:00:01\r",
      "\u001b[K    62% |████████████████████▏           | 819kB 52.4MB/s eta 0:00:01\r",
      "\u001b[K    63% |████████████████████▍           | 829kB 52.2MB/s eta 0:00:01\r",
      "\u001b[K    64% |████████████████████▋           | 839kB 52.4MB/s eta 0:00:01\r",
      "\u001b[K    65% |█████████████████████           | 849kB 53.2MB/s eta 0:00:01\r",
      "\u001b[K    66% |█████████████████████▏          | 860kB 50.4MB/s eta 0:00:01\r",
      "\u001b[K    66% |█████████████████████▍          | 870kB 50.6MB/s eta 0:00:01\r",
      "\u001b[K    67% |█████████████████████▋          | 880kB 51.7MB/s eta 0:00:01\r",
      "\u001b[K    68% |██████████████████████          | 890kB 52.0MB/s eta 0:00:01\r",
      "\u001b[K    69% |██████████████████████▏         | 901kB 51.5MB/s eta 0:00:01\r",
      "\u001b[K    70% |██████████████████████▍         | 911kB 52.4MB/s eta 0:00:01\r",
      "\u001b[K    70% |██████████████████████▊         | 921kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K    71% |███████████████████████         | 931kB 48.4MB/s eta 0:00:01\r",
      "\u001b[K    72% |███████████████████████▏        | 942kB 48.1MB/s eta 0:00:01\r",
      "\u001b[K    73% |███████████████████████▍        | 952kB 47.4MB/s eta 0:00:01\r",
      "\u001b[K    74% |███████████████████████▊        | 962kB 52.2MB/s eta 0:00:01\r",
      "\u001b[K    74% |████████████████████████        | 972kB 52.5MB/s eta 0:00:01\r",
      "\u001b[K    75% |████████████████████████▏       | 983kB 52.0MB/s eta 0:00:01\r",
      "\u001b[K    76% |████████████████████████▍       | 993kB 52.5MB/s eta 0:00:01\r",
      "\u001b[K    77% |████████████████████████▊       | 1.0MB 52.8MB/s eta 0:00:01\r",
      "\u001b[K    77% |█████████████████████████       | 1.0MB 52.7MB/s eta 0:00:01\r",
      "\u001b[K    78% |█████████████████████████▏      | 1.0MB 56.1MB/s eta 0:00:01\r",
      "\u001b[K    79% |█████████████████████████▍      | 1.0MB 57.6MB/s eta 0:00:01\r",
      "\u001b[K    80% |█████████████████████████▊      | 1.0MB 58.9MB/s eta 0:00:01\r",
      "\u001b[K    81% |██████████████████████████      | 1.1MB 60.1MB/s eta 0:00:01\r",
      "\u001b[K    81% |██████████████████████████▏     | 1.1MB 58.7MB/s eta 0:00:01\r",
      "\u001b[K    82% |██████████████████████████▌     | 1.1MB 59.9MB/s eta 0:00:01\r",
      "\u001b[K    83% |██████████████████████████▊     | 1.1MB 60.3MB/s eta 0:00:01\r",
      "\u001b[K    84% |███████████████████████████     | 1.1MB 50.1MB/s eta 0:00:01\r",
      "\u001b[K    85% |███████████████████████████▏    | 1.1MB 49.4MB/s eta 0:00:01\r",
      "\u001b[K    85% |███████████████████████████▌    | 1.1MB 48.9MB/s eta 0:00:01\r",
      "\u001b[K    86% |███████████████████████████▊    | 1.1MB 48.9MB/s eta 0:00:01\r",
      "\u001b[K    87% |████████████████████████████    | 1.1MB 48.8MB/s eta 0:00:01\r",
      "\u001b[K    88% |████████████████████████████▏   | 1.1MB 48.3MB/s eta 0:00:01\r",
      "\u001b[K    88% |████████████████████████████▌   | 1.2MB 48.5MB/s eta 0:00:01\r",
      "\u001b[K    89% |████████████████████████████▊   | 1.2MB 48.4MB/s eta 0:00:01\r",
      "\u001b[K    90% |█████████████████████████████   | 1.2MB 48.2MB/s eta 0:00:01\r",
      "\u001b[K    91% |█████████████████████████████▎  | 1.2MB 46.1MB/s eta 0:00:01\r",
      "\u001b[K    92% |█████████████████████████████▌  | 1.2MB 53.7MB/s eta 0:00:01\r",
      "\u001b[K    92% |█████████████████████████████▊  | 1.2MB 55.4MB/s eta 0:00:01\r",
      "\u001b[K    93% |██████████████████████████████  | 1.2MB 54.9MB/s eta 0:00:01\r",
      "\u001b[K    94% |██████████████████████████████▎ | 1.2MB 54.6MB/s eta 0:00:01\r",
      "\u001b[K    95% |██████████████████████████████▌ | 1.2MB 54.5MB/s eta 0:00:01\r",
      "\u001b[K    96% |██████████████████████████████▊ | 1.2MB 53.4MB/s eta 0:00:01\r",
      "\u001b[K    96% |███████████████████████████████ | 1.3MB 52.6MB/s eta 0:00:01\r",
      "\u001b[K    97% |███████████████████████████████▎| 1.3MB 52.7MB/s eta 0:00:01\r",
      "\u001b[K    98% |███████████████████████████████▌| 1.3MB 52.0MB/s eta 0:00:01\r",
      "\u001b[K    99% |███████████████████████████████▊| 1.3MB 54.0MB/s eta 0:00:01\r",
      "\u001b[K    99% |████████████████████████████████| 1.3MB 54.1MB/s eta 0:00:01\r",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 15.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.1.0)\n",
      "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.22.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend) (40.6.2)\n",
      "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.14.6)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2.5.3)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2018.7)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.3.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2RMFEDH0KT4"
   },
   "source": [
    "# *Libraries*\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_cV6beb2yKxW",
    "outputId": "7e461df0-1766-4a16-8641-7ac6b1b5e32f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZBY_elR0Edb"
   },
   "source": [
    "# MNIST Data\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiDVLaSUygDx"
   },
   "outputs": [],
   "source": [
    "with gzip.open('mnist.pkl.gz','rb') as ff :\n",
    "    u = pickle._Unpickler( ff )\n",
    "    u.encoding = 'latin1'\n",
    "    train, val, test = u.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "H1Yhvjqt0tLO",
    "outputId": "51176563-028b-49e7-de79-5870e26f2458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print( train[0].shape, train[1].shape )\n",
    "print( val[0].shape, val[1].shape )\n",
    "print( test[0].shape, test[1].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0PyCEd7019w"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_features = 784\n",
    "eta = 0.02\n",
    "final=[]\n",
    "final_image=[]\n",
    "input_size = 784\n",
    "drop_out = 0.1\n",
    "first_dense_layer_nodes  = 1568\n",
    "second_dense_layer_nodes = 10\n",
    "validation_data_split = 0.0\n",
    "num_epochs = 50\n",
    "model_batch_size = 1024\n",
    "tb_batch_size = 32\n",
    "early_patience = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDZu8S-t0Y4H"
   },
   "source": [
    "## Logistic Reression I\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPQn6XsE06mL"
   },
   "outputs": [],
   "source": [
    "def softmax(Z, epsilon=1e-9):\n",
    "    e = np.exp(Z - np.max(Z))\n",
    "    # very tiny epsilon to the result of softmax function output, to avoid applying logarithm to zeros\n",
    "    if e.ndim == 1:\n",
    "        return e / np.sum(e, axis=0) + epsilon\n",
    "    else:  \n",
    "        return e / np.array([np.sum(e, axis=1)]).T + epsilon\n",
    "\n",
    "def infer(W, X):\n",
    "    #use of np.hstack to add additional column of ones to X;\n",
    "\n",
    "    X_ones = np.hstack((X, np.ones(((X.shape[0]), 1))))\n",
    "    XW = np.dot(X_ones, W)\n",
    "    smax = softmax(XW)\n",
    "    return smax\n",
    "\n",
    "def one_hot_encode(labels_list, max_number):\n",
    "   \n",
    "    b= np_utils.to_categorical(np.array(labels_list),max_number)\n",
    "    return b\n",
    "\n",
    "def loss(W, X, Y):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    T = infer(W, X)    \n",
    "    return (-1 / m) * np.sum(np.log(T) * Y) + eta / 2 * np.sum(W * W)\n",
    "\n",
    "y_onehot = one_hot_encode(train[1], num_classes)\n",
    "                          \n",
    "def get_grad(W, X, Y):\n",
    "  \n",
    "    X_alt = np.hstack((X, np.ones(((X.shape[0]), 1))))\n",
    "    m = X.shape[0]\n",
    "    T = infer(W, X)   \n",
    "    return (-1 / m) * np.dot(X_alt.T, (Y - T)) + eta * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spA8ziTc0_iP"
   },
   "outputs": [],
   "source": [
    "def training(X_train, y_train, batch_size=100, num_epoch=1, n_classes=num_classes, lr=0.001, plot_loss=True):\n",
    "    \n",
    "    #Perform gradient descent and return trained weights;\n",
    "        \n",
    "    losses = []\n",
    "    n_features = num_features\n",
    "    w = np.random.randn(n_features+1, n_classes)/n_features\n",
    "    for epoch in range(num_epoch):\n",
    "#         grad = get_grad(w, X_train, one_hot_encode(y_train, n_classes))\n",
    "#         # print(grad)\n",
    "#         w = w - step * grad\n",
    "#         # print(w)\n",
    "#         losses.append(loss(w, X_train, one_hot_encode(y_train, n_classes)))\n",
    "        \n",
    "        for iter_num, (x_batch, y_batch) in enumerate(zip(np.split(X_train, batch_size), np.split(y_train, batch_size))):\n",
    "            grad = get_grad(w, x_batch, one_hot_encode(y_batch, n_classes))\n",
    "            gradient_step = lr * grad\n",
    "            # print(gradient_step)\n",
    "            w -= gradient_step\n",
    "            # print(w)\n",
    "            losses.append(loss(w, x_batch, one_hot_encode(y_batch, n_classes)))\n",
    "            \n",
    "    if plot_loss:\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"epoch*batches\")\n",
    "        plt.show()\n",
    "        \n",
    "    return w\n",
    "\n",
    "def make_prediction(X, W):\n",
    "\n",
    "    probability_matrix = infer(W, X)\n",
    "    return np.array([np.argmax(t) for t in probability_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "lzdHaSEE1Pwu",
    "outputId": "8a26ebb7-add4-4a34-a52b-a9a2df1a5800"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05640b7b00>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 82.18858504295349 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "W = training(train[0], train[1], num_epoch=100)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "colab_type": "code",
    "id": "AwmhFGlLOXap",
    "outputId": "2e086f6a-2d75-4382-eff3-7a472f730617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Logistic Regression I\n",
      "Accuracy: 87.51\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.94      0.93       991\n",
      "          1       0.86      0.96      0.91      1064\n",
      "          2       0.89      0.84      0.86       990\n",
      "          3       0.84      0.87      0.85      1030\n",
      "          4       0.89      0.88      0.89       983\n",
      "          5       0.90      0.75      0.82       915\n",
      "          6       0.91      0.94      0.92       967\n",
      "          7       0.91      0.88      0.90      1090\n",
      "          8       0.84      0.82      0.83      1009\n",
      "          9       0.81      0.86      0.83       961\n",
      "\n",
      "avg / total       0.88      0.88      0.87     10000\n",
      "\n",
      "[[ 934    0    7    6    1    1   18    3   17    4]\n",
      " [   0 1024    5    5    1    6    3    2   15    3]\n",
      " [  11   20  832   15   19    2   24   24   32   11]\n",
      " [   5    9   23  894    1   30    8    3   42   15]\n",
      " [   0   15    4    0  867    1    8    2   12   74]\n",
      " [  17   20   11   82   21  682   26    6   22   28]\n",
      " [   8   11   14    1    8   10  907    0    8    0]\n",
      " [  19   33   17    3   16    0    0  964    6   32]\n",
      " [   5   45   17   44    3   19    7   14  824   31]\n",
      " [  10   15   10   18   38    5    0   37    5  823]]\n",
      "Testing Logistic Regression I\n",
      "Accuracy: 87.07000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.97      0.94       980\n",
      "          1       0.90      0.97      0.93      1135\n",
      "          2       0.89      0.82      0.85      1032\n",
      "          3       0.84      0.87      0.86      1010\n",
      "          4       0.86      0.87      0.87       982\n",
      "          5       0.88      0.72      0.79       892\n",
      "          6       0.88      0.92      0.90       958\n",
      "          7       0.90      0.86      0.88      1028\n",
      "          8       0.81      0.83      0.82       974\n",
      "          9       0.83      0.85      0.84      1009\n",
      "\n",
      "avg / total       0.87      0.87      0.87     10000\n",
      "\n",
      "[[ 947    0    3    2    0    3   16    1    8    0]\n",
      " [   0 1096    5    3    1    2    4    0   24    0]\n",
      " [  16   22  845   26   19    0   28   22   47    7]\n",
      " [   5    3   22  881    1   29    8   20   27   14]\n",
      " [   3    9    5    0  857    1   17    2   11   77]\n",
      " [  26   15    6   82   24  643   29    9   41   17]\n",
      " [  19    5   13    2   13   18  882    0    6    0]\n",
      " [   4   39   26    1   13    0    4  889   10   42]\n",
      " [   9   17   14   39   12   22   18   14  809   20]\n",
      " [  14   13   11   12   52   10    1   27   11  858]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Logistic Regression I\")\n",
    "right=0\n",
    "wrong=0\n",
    "y_pred = make_prediction(val[0], W)\n",
    "for i,j in zip(val[1],y_pred):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(classification_report(val[1], y_pred))\n",
    "print(confusion_matrix(val[1], y_pred))\n",
    "print(\"Testing Logistic Regression I\")\n",
    "\n",
    "right=0\n",
    "wrong=0\n",
    "y_pred = make_prediction(test[0], W)\n",
    "for i,j in zip(test[1],y_pred):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(classification_report(test[1], y_pred))\n",
    "print(confusion_matrix(test[1], y_pred))\n",
    "final.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GiKXugS0mwr"
   },
   "source": [
    "## Neural Network I\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1767
    },
    "colab_type": "code",
    "id": "81Um4iSxPZlP",
    "outputId": "587c64b4-3fbe-45c1-e4ab-932831138746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 2.5007 - acc: 0.2368\n",
      "Epoch 2/50\n",
      " 7168/50000 [===>..........................] - ETA: 0s - loss: 1.8806 - acc: 0.3916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 22us/step - loss: 1.4296 - acc: 0.5457\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.9250 - acc: 0.6893\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.6547 - acc: 0.7846\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.5559 - acc: 0.8200\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.4866 - acc: 0.8458\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.4379 - acc: 0.8660\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.4274 - acc: 0.8675\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3948 - acc: 0.8800\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3869 - acc: 0.8838\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3722 - acc: 0.8872\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3604 - acc: 0.8911\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3629 - acc: 0.8888\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3413 - acc: 0.8983\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3420 - acc: 0.8968\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3340 - acc: 0.8997\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3300 - acc: 0.9003\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3252 - acc: 0.9034\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3113 - acc: 0.9070\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3110 - acc: 0.9072\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3082 - acc: 0.9086\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.3002 - acc: 0.9112\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2980 - acc: 0.9095\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2949 - acc: 0.9126\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2867 - acc: 0.9147\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2835 - acc: 0.9155\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2782 - acc: 0.9176\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2746 - acc: 0.9184\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2719 - acc: 0.9195\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2661 - acc: 0.9208\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2669 - acc: 0.9204\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2600 - acc: 0.9223\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2620 - acc: 0.9204\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2541 - acc: 0.9239\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2488 - acc: 0.9259\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2484 - acc: 0.9245\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2421 - acc: 0.9274\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2406 - acc: 0.9274\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2358 - acc: 0.9292\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2323 - acc: 0.9306\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2292 - acc: 0.9311\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2245 - acc: 0.9326\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2230 - acc: 0.9330\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2222 - acc: 0.9332\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2159 - acc: 0.9352\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2136 - acc: 0.9359\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2093 - acc: 0.9375\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2087 - acc: 0.9368\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2049 - acc: 0.9385\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 1s 22us/step - loss: 0.2009 - acc: 0.9389\n",
      "--- 56.20131778717041 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    model.add(Dense(first_dense_layer_nodes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(drop_out))\n",
    "\n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    model.add(Activation('softmax'))\n",
    "   \n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adadelta',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "start_time = time.time()\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=0, patience=early_patience, mode='min')\n",
    "history = model.fit((train[0])\n",
    "                    , y_onehot\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "colab_type": "code",
    "id": "vLg9fkdYQl5M",
    "outputId": "293833c7-32d9-4040-a5bb-bf1aab7732cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation NN I\n",
      "Accuracy: 94.62\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.96      0.97       991\n",
      "          1       0.94      0.98      0.96      1064\n",
      "          2       0.96      0.94      0.95       990\n",
      "          3       0.91      0.93      0.92      1030\n",
      "          4       0.96      0.95      0.96       983\n",
      "          5       0.89      0.93      0.91       915\n",
      "          6       0.97      0.98      0.97       967\n",
      "          7       0.96      0.97      0.97      1090\n",
      "          8       0.96      0.89      0.92      1009\n",
      "          9       0.93      0.93      0.93       961\n",
      "\n",
      "avg / total       0.95      0.95      0.95     10000\n",
      "\n",
      "[[ 956    0    8    2    1    9    6    1    4    4]\n",
      " [   0 1048    3    6    1    3    0    0    2    1]\n",
      " [   2    5  932   11    6    7    6    9    8    4]\n",
      " [   2    4    5  958    1   39    1    3   10    7]\n",
      " [   1   10    2    1  932    1    2    2    2   30]\n",
      " [   4    1    7   25    3  851   13    2    5    4]\n",
      " [   2    3    4    0    4    9  944    0    1    0]\n",
      " [   1    7    6    6    2    0    0 1057    0   11]\n",
      " [   2   32    6   28    1   28    1    5  895   11]\n",
      " [   4    6    1   14   17    7    1   19    3  889]]\n",
      "Testing NN I\n",
      "Accuracy: 94.06\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97       980\n",
      "          1       0.96      0.99      0.97      1135\n",
      "          2       0.95      0.94      0.95      1032\n",
      "          3       0.90      0.95      0.92      1010\n",
      "          4       0.95      0.93      0.94       982\n",
      "          5       0.89      0.92      0.90       892\n",
      "          6       0.95      0.95      0.95       958\n",
      "          7       0.95      0.94      0.95      1028\n",
      "          8       0.95      0.87      0.91       974\n",
      "          9       0.93      0.93      0.93      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "[[ 958    0    1    2    0    9    5    3    1    1]\n",
      " [   0 1121    2    2    0    2    3    2    3    0]\n",
      " [   6    4  969   11    7    4    9   11    9    2]\n",
      " [   0    1   10  959    0   22    1    9    6    2]\n",
      " [   1    4    7    0  916    1    7    4    4   38]\n",
      " [   8    2    0   27    3  817   10    2   15    8]\n",
      " [  11    3    4    3    9   17  909    0    2    0]\n",
      " [   1   10   18    6    3    2    0  969    2   17]\n",
      " [   3   10    4   40    6   38    9   11  849    4]\n",
      " [   5   10    1   15   18    9    1    9    2  939]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation NN I\")\n",
    "\n",
    "right=0\n",
    "wrong=0\n",
    "predictedTestLabel=[]\n",
    "for i,j in zip((val[0]),val[1]):\n",
    "    y = model.predict(np.array(i).reshape(-1,784))\n",
    "    \n",
    "    predictedTestLabel.append(y.argmax())\n",
    "    \n",
    "    if j == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "#print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(val[1], predictedTestLabel))\n",
    "print(confusion_matrix(val[1], predictedTestLabel))\n",
    "\n",
    "print(\"Testing NN I\")\n",
    "\n",
    "right=0\n",
    "wrong=0\n",
    "predictedTestLabel=[]\n",
    "for i,j in zip((test[0]),test[1]):\n",
    "    y = model.predict(np.array(i).reshape(-1,784))\n",
    "    \n",
    "    predictedTestLabel.append(y.argmax())\n",
    "    \n",
    "    if j == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "#print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(test[1], predictedTestLabel))\n",
    "print(confusion_matrix(test[1], predictedTestLabel))\n",
    "final.append(predictedTestLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJq_xzL6RUYy"
   },
   "outputs": [],
   "source": [
    "def accuracy(clf,train,test):\n",
    "  right=0\n",
    "  wrong=0\n",
    "  y_pred = clf.predict(train)\n",
    "  for i,j in zip(test,y_pred):\n",
    "      if i==j:\n",
    "          right = right + 1\n",
    "      else:\n",
    "          wrong = wrong + 1\n",
    "  print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "  print(classification_report(test, y_pred))\n",
    "  print(confusion_matrix(test, y_pred))\n",
    "  final.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcorNL7o0yhV"
   },
   "source": [
    "## Logistic Regression II & III\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "e3X-IDQ0RxRP",
    "outputId": "c0fbd53a-9f28-415b-83df-1709dfc4a325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 79.29714560508728 seconds ---\n",
      "--- 39.824546098709106 seconds ---\n"
     ]
    }
   ],
   "source": [
    "clf_LR1 = LogisticRegression(random_state=0, solver='lbfgs',multi_class='ovr')\n",
    "clf_LR2 = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "start_time = time.time()\n",
    "clf_LR1.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "clf_LR2.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "m6vHXSNFRt55",
    "outputId": "440b64fe-4d1a-4ab8-9836-ae355cd2bc28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.06\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       980\n",
      "          1       0.96      0.98      0.97      1135\n",
      "          2       0.94      0.89      0.91      1032\n",
      "          3       0.89      0.91      0.90      1010\n",
      "          4       0.92      0.94      0.93       982\n",
      "          5       0.90      0.86      0.88       892\n",
      "          6       0.94      0.95      0.94       958\n",
      "          7       0.93      0.92      0.93      1028\n",
      "          8       0.87      0.88      0.87       974\n",
      "          9       0.91      0.88      0.89      1009\n",
      "\n",
      "avg / total       0.92      0.92      0.92     10000\n",
      "\n",
      "[[ 961    0    1    2    0    4    6    3    1    2]\n",
      " [   0 1112    2    1    0    1    4    1   14    0]\n",
      " [   8    8  921   18   10    5   12   10   37    3]\n",
      " [   4    1   19  919    3   21    4   12   19    8]\n",
      " [   1    2    3    3  921    0    9    1    6   36]\n",
      " [  10    3    1   40   10  768   16    6   30    8]\n",
      " [  10    3    7    2    5   18  910    0    3    0]\n",
      " [   3    9   21    6    7    1    1  949    5   26]\n",
      " [   9   14    6   23   13   27    8   11  853   10]\n",
      " [   8    8    2   15   33   11    0   27   13  892]]\n",
      "Accuracy: 92.54\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97       980\n",
      "          1       0.96      0.98      0.97      1135\n",
      "          2       0.93      0.90      0.91      1032\n",
      "          3       0.91      0.91      0.91      1010\n",
      "          4       0.94      0.93      0.93       982\n",
      "          5       0.90      0.87      0.89       892\n",
      "          6       0.93      0.95      0.94       958\n",
      "          7       0.93      0.93      0.93      1028\n",
      "          8       0.88      0.88      0.88       974\n",
      "          9       0.91      0.91      0.91      1009\n",
      "\n",
      "avg / total       0.93      0.93      0.93     10000\n",
      "\n",
      "[[ 958    0    0    2    1    7    6    5    1    0]\n",
      " [   0 1113    3    1    0    2    4    2   10    0]\n",
      " [   4   10  931   16    5    4   16    9   33    4]\n",
      " [   4    1   18  918    2   23    4   11   21    8]\n",
      " [   1    2    6    3  912    0    9    5    8   36]\n",
      " [  10    3    4   36    8  777   13    5   30    6]\n",
      " [   9    3    9    1    7   13  912    3    1    0]\n",
      " [   1    7   23    9    6    1    0  952    3   26]\n",
      " [   8   10    8   19    7   27   14    8  859   14]\n",
      " [  10    8    1    9   25    5    0   21    8  922]]\n"
     ]
    }
   ],
   "source": [
    "accuracy(clf_LR1,test[0],test[1])\n",
    "accuracy(clf_LR2,test[0],test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmYsbzWp05yC"
   },
   "source": [
    "## Neural Network II & III\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "IVCPStrYVLAB",
    "outputId": "188c3473-7f16-425a-e647-5e38d87a73a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 188.56582903862 seconds ---\n",
      "--- 166.13437151908875 seconds ---\n"
     ]
    }
   ],
   "source": [
    "clf_NN1 = MLPClassifier(solver='adam',alpha=0.01,learning_rate='adaptive',hidden_layer_sizes=(256, 2),random_state=1,max_iter=50)  \n",
    "clf_NN2 = MLPClassifier(solver='sgd',alpha=0.01,learning_rate='adaptive',hidden_layer_sizes=(256, 2),random_state=1,max_iter=50)    \n",
    "start_time = time.time()\n",
    "clf_NN1.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "clf_NN2.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "up1LDZz8Y5SM",
    "outputId": "6b315f89-f817-461a-88b1-9b6e9e38a52b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.4\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.96      0.93       980\n",
      "          1       0.96      0.98      0.97      1135\n",
      "          2       0.92      0.90      0.91      1032\n",
      "          3       0.93      0.91      0.92      1010\n",
      "          4       0.98      0.96      0.97       982\n",
      "          5       0.96      0.91      0.93       892\n",
      "          6       0.94      0.96      0.95       958\n",
      "          7       0.92      0.94      0.93      1028\n",
      "          8       0.88      0.91      0.89       974\n",
      "          9       0.95      0.92      0.93      1009\n",
      "\n",
      "avg / total       0.93      0.93      0.93     10000\n",
      "\n",
      "[[ 938    0   14   14    0    0    1    3   10    0]\n",
      " [   1 1108    1    0    0    1    5    0   19    0]\n",
      " [  35    2  931    7    0   28    1    1   27    0]\n",
      " [  45    0    7  921    0    0    0   21   14    2]\n",
      " [   0    0    0    0  941    0   10    5    3   23]\n",
      " [   6   19   36    3    0  808    5    2   13    0]\n",
      " [   1    9    4    1    5    1  923    0   14    0]\n",
      " [   3    1    2   28    0    0    1  963   15   15]\n",
      " [   2   18   14    7    0    4   29   13  883    4]\n",
      " [   2    3    1   14   18    0    3   38    6  924]]\n",
      "Accuracy: 84.33\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.89      0.94       980\n",
      "          1       0.88      0.98      0.92      1135\n",
      "          2       0.81      0.85      0.83      1032\n",
      "          3       0.87      0.83      0.85      1010\n",
      "          4       0.92      0.92      0.92       982\n",
      "          5       0.72      0.58      0.64       892\n",
      "          6       0.92      0.92      0.92       958\n",
      "          7       0.80      0.86      0.83      1028\n",
      "          8       0.63      0.69      0.66       974\n",
      "          9       0.89      0.86      0.87      1009\n",
      "\n",
      "avg / total       0.85      0.84      0.84     10000\n",
      "\n",
      "[[ 871   63   36    0    0    0    7    1    2    0]\n",
      " [   0 1108   11    0    0    2    6    3    5    0]\n",
      " [  10   33  876    0    0   56   17   12   28    0]\n",
      " [   0    0    4  843    0   12    0   27  108   16]\n",
      " [   0    1    1    0  907    0   13   31    2   27]\n",
      " [   1    8   95   18    3  517    9   23  217    1]\n",
      " [   1   41   20    0    8    0  879    8    1    0]\n",
      " [   0    2   10    6   15    5   14  889   28   59]\n",
      " [   0    3   22   76    4  122    7   60  676    4]\n",
      " [   0    3    7   21   48    2    1   51    9  867]]\n"
     ]
    }
   ],
   "source": [
    "accuracy(clf_NN1,test[0],test[1])\n",
    "accuracy(clf_NN2,test[0],test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnmbRQ3Rtd3H"
   },
   "source": [
    "## Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYcRz9IhtbVJ"
   },
   "outputs": [],
   "source": [
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "history = AccuracyHistory()\n",
    "\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "CNN_model = Sequential()\n",
    "CNN_model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='sigmoid',\n",
    "                 input_shape=(28,28,1)))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "CNN_model.add(Conv2D(64, (5, 5), activation='sigmoid'))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(1000, activation='relu'))\n",
    "CNN_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "(X_train, y_train)=(train[0],one_hot_encode(train[1], num_classes))\n",
    "(X_test, y_test)=(test[0],one_hot_encode(test[1], num_classes))\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "CNN_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPRf6QXhu0xj"
   },
   "outputs": [],
   "source": [
    "CNN_model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=32,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfFZaW9Wu5-g"
   },
   "outputs": [],
   "source": [
    "score = CNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred=CNN_model.predict(X_test)\n",
    "y_pred=np.array([np.argmax(t) for t in y_pred])\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test[1],y_pred):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(classification_report(test[1], y_pred))\n",
    "print(confusion_matrix(test[1], y_pred))\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Tq1wfj91BfS"
   },
   "source": [
    "## Random Forest \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "r8MuZW3RZYJG",
    "outputId": "c2478604-fe4b-413f-80e2-0d5e96dab6c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.440720796585083 seconds ---\n",
      "--- 44.53623819351196 seconds ---\n",
      "--- 89.7226333618164 seconds ---\n"
     ]
    }
   ],
   "source": [
    "clf_RBF1 = RandomForestClassifier(n_estimators=10)\n",
    "clf_RBF2 = RandomForestClassifier(n_estimators=100)\n",
    "clf_RBF3 = RandomForestClassifier(n_estimators=200)\n",
    "start_time = time.time()\n",
    "clf_RBF1.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "clf_RBF2.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "clf_RBF3.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1327
    },
    "colab_type": "code",
    "id": "J3IcRAP4bCGO",
    "outputId": "d30e27e3-0ab0-4fcd-fbc6-5e2f7f5a2111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.45\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97       980\n",
      "          1       0.98      0.99      0.98      1135\n",
      "          2       0.91      0.96      0.94      1032\n",
      "          3       0.92      0.94      0.93      1010\n",
      "          4       0.94      0.95      0.94       982\n",
      "          5       0.94      0.91      0.93       892\n",
      "          6       0.97      0.96      0.96       958\n",
      "          7       0.96      0.93      0.95      1028\n",
      "          8       0.94      0.90      0.92       974\n",
      "          9       0.94      0.92      0.93      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "[[ 967    1    2    1    1    2    2    1    3    0]\n",
      " [   0 1119    2    6    1    3    2    1    1    0]\n",
      " [   7    1  991    5    4    2    3    7   10    2]\n",
      " [   6    0   20  949    1   15    1    8   10    0]\n",
      " [   1    3    2    1  930    0    8    0    6   31]\n",
      " [  10    1    6   29    3  816    6    6   12    3]\n",
      " [  13    4    6    1    7    5  915    1    4    2]\n",
      " [   3    7   25    7    9    2    0  959    1   15]\n",
      " [  10    2   25   25    9   17    3    4  874    5]\n",
      " [   6    5    8    9   27    9    3   10    7  925]]\n",
      "Accuracy: 96.86\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.96      0.97      0.96      1032\n",
      "          3       0.96      0.96      0.96      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.97      0.96      0.97       892\n",
      "          6       0.97      0.98      0.98       958\n",
      "          7       0.97      0.96      0.97      1028\n",
      "          8       0.96      0.95      0.95       974\n",
      "          9       0.96      0.95      0.95      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n",
      "[[ 971    0    1    0    0    1    3    1    3    0]\n",
      " [   0 1124    3    2    0    2    2    1    1    0]\n",
      " [   5    0 1000    5    2    0    3   10    7    0]\n",
      " [   0    0   11  971    0    9    0    9    8    2]\n",
      " [   1    0    1    0  950    0    7    0    2   21]\n",
      " [   3    0    1   11    2  859    6    1    6    3]\n",
      " [   5    3    1    0    5    3  937    0    4    0]\n",
      " [   0    3   18    2    2    0    0  991    1   11]\n",
      " [   3    1    5   10    7    7    3    4  926    8]\n",
      " [   7    5    3   10   10    2    1    6    8  957]]\n",
      "Accuracy: 96.87\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.96      0.97      0.97      1032\n",
      "          3       0.95      0.97      0.96      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.98      0.96      0.97       892\n",
      "          6       0.98      0.98      0.98       958\n",
      "          7       0.97      0.96      0.97      1028\n",
      "          8       0.96      0.95      0.96       974\n",
      "          9       0.95      0.95      0.95      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n",
      "[[ 969    0    0    0    0    1    4    1    4    1]\n",
      " [   0 1121    3    4    1    2    2    0    1    1]\n",
      " [   5    0 1000    6    3    0    2    9    7    0]\n",
      " [   0    0    9  976    0    6    0    8    8    3]\n",
      " [   1    0    0    0  955    0    5    0    3   18]\n",
      " [   4    0    1   16    3  852    6    3    6    1]\n",
      " [   6    3    0    1    3    5  937    0    3    0]\n",
      " [   1    4   19    1    1    0    0  989    1   12]\n",
      " [   5    0    5    7    7    5    3    4  928   10]\n",
      " [   5    5    1   15   10    2    1    4    6  960]]\n"
     ]
    }
   ],
   "source": [
    "accuracy(clf_RBF1,test[0],test[1])\n",
    "accuracy(clf_RBF2,test[0],test[1])\n",
    "accuracy(clf_RBF3,test[0],test[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "8RmZhGp5ckNm",
    "outputId": "c589c2bd-0dd5-4798-a132-b244c5e6b0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10000)\n",
      "Errors: 376  Correct :9624\n",
      "Accuracy: 96.24000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98       980\n",
      "          1       0.98      0.99      0.99      1135\n",
      "          2       0.95      0.97      0.96      1032\n",
      "          3       0.94      0.96      0.95      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.96      0.94      0.95       892\n",
      "          6       0.97      0.97      0.97       958\n",
      "          7       0.96      0.96      0.96      1028\n",
      "          8       0.95      0.93      0.94       974\n",
      "          9       0.97      0.94      0.95      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "[[ 970    0    1    1    0    1    4    1    2    0]\n",
      " [   0 1124    2    2    0    1    3    1    2    0]\n",
      " [   4    3  996    6    2    0    5    8    8    0]\n",
      " [   3    0   12  971    0    8    0    9    7    0]\n",
      " [   1    1    3    0  957    0    5    0    2   13]\n",
      " [   8    1    1   19    3  836    6    1   15    2]\n",
      " [   8    3    3    1    5    6  930    0    2    0]\n",
      " [   2    6   19    4    2    1    0  984    1    9]\n",
      " [   5    2    5   12    7   14    8    7  909    5]\n",
      " [   8    7    2   13   13    4    1   10    4  947]]\n"
     ]
    }
   ],
   "source": [
    "m = stats.mode(np.array(final))\n",
    "print(np.array(final).shape)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test[1], m[0][0]):\n",
    "    \n",
    "    if i == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(test[1], m[0][0]))\n",
    "print(confusion_matrix(test[1], m[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3paFO8Qo1ICf"
   },
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "iJ_Exk3jbWE9",
    "outputId": "e4318865-5d75-494e-92ef-6229f4131de5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 323.8312437534332 seconds ---\n",
      "--- 563.8215866088867 seconds ---\n"
     ]
    }
   ],
   "source": [
    "clf1 = SVC(kernel='linear')\n",
    "clf2 = SVC(kernel='rbf', gamma=1)\n",
    "clf3 = SVC(kernel='rbf')\n",
    "start_time = time.time()\n",
    "clf1.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "clf3.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtFzo9jGY0Gt"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "clf2.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "accuracy(clf2,test[0],test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "Sx1ewiWarKLl",
    "outputId": "c6defe2b-c88e-452a-bba6-db440b3468f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.89999999999999\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       980\n",
      "          1       0.97      0.99      0.98      1135\n",
      "          2       0.92      0.94      0.93      1032\n",
      "          3       0.90      0.93      0.92      1010\n",
      "          4       0.93      0.96      0.95       982\n",
      "          5       0.92      0.89      0.91       892\n",
      "          6       0.96      0.95      0.95       958\n",
      "          7       0.95      0.93      0.94      1028\n",
      "          8       0.93      0.89      0.91       974\n",
      "          9       0.95      0.91      0.93      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "[[ 959    0    5    2    2    4    7    0    1    0]\n",
      " [   0 1121    3    3    0    1    2    1    4    0]\n",
      " [   6    8  968    9    3    2   11   10   13    2]\n",
      " [   5    2   17  944    4   13    1    8   13    3]\n",
      " [   2    1   10    1  943    0    4    2    2   17]\n",
      " [  13    4    2   39    5  792    9    1   22    5]\n",
      " [  10    3   11    1    5   14  911    2    1    0]\n",
      " [   1    8   20   10    6    1    0  961    3   18]\n",
      " [   8    4    9   25   11   27    6    5  871    8]\n",
      " [   7    6    2   13   32    4    0   18    7  920]]\n",
      "Accuracy: 94.35\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97       980\n",
      "          1       0.97      0.99      0.98      1135\n",
      "          2       0.94      0.93      0.94      1032\n",
      "          3       0.93      0.94      0.93      1010\n",
      "          4       0.93      0.95      0.94       982\n",
      "          5       0.93      0.91      0.92       892\n",
      "          6       0.95      0.96      0.96       958\n",
      "          7       0.95      0.93      0.94      1028\n",
      "          8       0.94      0.91      0.93       974\n",
      "          9       0.94      0.91      0.93      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "[[ 967    0    1    0    0    5    4    1    2    0]\n",
      " [   0 1120    2    3    0    1    3    1    5    0]\n",
      " [   9    1  962    7   10    1   13   11   16    2]\n",
      " [   1    1   14  950    1   17    1   10   11    4]\n",
      " [   1    1    7    0  937    0    7    2    2   25]\n",
      " [   7    4    5   33    7  808   11    2   10    5]\n",
      " [  10    3    4    1    5   10  924    0    1    0]\n",
      " [   2   13   22    5    7    1    0  954    4   20]\n",
      " [   4    6    6   14    8   24   10    8  891    3]\n",
      " [  10    6    0   12   33    5    1   14    6  922]]\n"
     ]
    }
   ],
   "source": [
    "accuracy(clf1,test[0],test[1])\n",
    "accuracy(clf3,test[0],test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c954O8zE1pxr"
   },
   "source": [
    "### Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "JudX0GgDuvx3",
    "outputId": "9f5257ac-d520-4629-859c-b25ab0451a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n",
      "Errors: 400  Correct :9600\n",
      "Accuracy: 96.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98       980\n",
      "          1       0.98      0.99      0.98      1135\n",
      "          2       0.95      0.96      0.96      1032\n",
      "          3       0.94      0.96      0.95      1010\n",
      "          4       0.96      0.97      0.97       982\n",
      "          5       0.95      0.93      0.94       892\n",
      "          6       0.96      0.97      0.97       958\n",
      "          7       0.96      0.96      0.96      1028\n",
      "          8       0.96      0.93      0.94       974\n",
      "          9       0.97      0.94      0.95      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "[[ 970    0    1    1    0    2    4    1    1    0]\n",
      " [   0 1123    2    2    0    1    3    1    3    0]\n",
      " [   6    2  994    4    3    1    7    8    6    1]\n",
      " [   0    0   12  971    0    9    0   11    7    0]\n",
      " [   1    1    4    0  955    0    6    0    2   13]\n",
      " [   9    3    1   24    3  829    7    1   12    3]\n",
      " [   8    3    3    1    5    6  930    0    2    0]\n",
      " [   2    7   20    3    3    0    0  982    1   10]\n",
      " [   5    3    5   15    8   18    8    6  902    4]\n",
      " [   8    6    1   14   15    3    1   13    4  944]]\n"
     ]
    }
   ],
   "source": [
    "m = stats.mode(np.array(final))\n",
    "print(np.array(final).shape)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test[1], m[0][0]):\n",
    "    \n",
    "    if i == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(test[1], m[0][0]))\n",
    "print(confusion_matrix(test[1], m[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UO1IOdIp1OQJ"
   },
   "source": [
    "## Bagging\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "vVckiwhvseAN",
    "outputId": "c7f42482-d739-4e48-ee8a-e438557ba460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 166.31701064109802 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 639.8875625133514 seconds ---\n",
      "--- 12.972444534301758 seconds ---\n",
      "--- 388.4729747772217 seconds ---\n"
     ]
    }
   ],
   "source": [
    "bagging1 = BaggingClassifier(base_estimator=clf_LR2, n_estimators=5, max_samples=0.8, max_features=0.8)\n",
    "bagging2 = BaggingClassifier(base_estimator=clf_NN1, n_estimators=5, max_samples=0.8, max_features=0.8)\n",
    "bagging3 = BaggingClassifier(base_estimator=clf_RBF1, n_estimators=5, max_samples=0.8, max_features=0.8)\n",
    "bagging4 = BaggingClassifier(base_estimator=clf1, n_estimators=5, max_samples=0.8, max_features=0.8)\n",
    "start_time = time.time()\n",
    "bagging1.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "bagging2.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "bagging3.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "bagging4.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1764
    },
    "colab_type": "code",
    "id": "LQ27TEIyu18-",
    "outputId": "3486806b-2d4b-432d-d271-50a8ab471a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.45\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       980\n",
      "          1       0.97      0.98      0.97      1135\n",
      "          2       0.93      0.90      0.91      1032\n",
      "          3       0.90      0.91      0.91      1010\n",
      "          4       0.93      0.93      0.93       982\n",
      "          5       0.91      0.86      0.88       892\n",
      "          6       0.94      0.95      0.94       958\n",
      "          7       0.93      0.93      0.93      1028\n",
      "          8       0.88      0.89      0.88       974\n",
      "          9       0.91      0.91      0.91      1009\n",
      "\n",
      "avg / total       0.92      0.92      0.92     10000\n",
      "\n",
      "[[ 961    0    0    2    0    8    4    4    1    0]\n",
      " [   0 1113    3    1    0    1    4    1   12    0]\n",
      " [   5    8  929   13   11    5   13    9   35    4]\n",
      " [   4    1   21  919    1   21    2   12   23    6]\n",
      " [   1    3    7    2  913    0    9    4    9   34]\n",
      " [  11    3    4   38    7  770   15    7   32    5]\n",
      " [  12    3    8    1    8   13  908    3    2    0]\n",
      " [   1    7   23    7    8    0    0  953    1   28]\n",
      " [   7    8    7   22    7   26   14    7  864   12]\n",
      " [  11    7    1   11   29    5    0   22    8  915]]\n",
      "Accuracy: 96.72\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98       980\n",
      "          1       0.98      0.99      0.98      1135\n",
      "          2       0.98      0.96      0.97      1032\n",
      "          3       0.97      0.96      0.97      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.99      0.95      0.97       892\n",
      "          6       0.99      0.97      0.98       958\n",
      "          7       0.97      0.96      0.97      1028\n",
      "          8       0.90      0.96      0.93       974\n",
      "          9       0.95      0.96      0.95      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n",
      "[[ 967    0    0    1    3    1    3    2    2    1]\n",
      " [   0 1121    2    0    1    3    1    1    6    0]\n",
      " [   5    4  995    3    8    0    0   12    2    3]\n",
      " [   0    0    4  974    0    1    0    6   24    1]\n",
      " [   1    0    1    0  957    0    1    0    7   15]\n",
      " [   1    2    0    3    1  846    5    0   26    8]\n",
      " [   6    8    0    1    6    1  930    0    6    0]\n",
      " [   0    4   11    5    0    0    0  984    6   18]\n",
      " [   3    5    2   10    5    5    0    2  932   10]\n",
      " [   2    2    0    5    7    0    0    3   24  966]]\n",
      "Accuracy: 96.11\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.95      0.96      0.95      1032\n",
      "          3       0.95      0.95      0.95      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.97      0.94      0.96       892\n",
      "          6       0.96      0.98      0.97       958\n",
      "          7       0.97      0.95      0.96      1028\n",
      "          8       0.95      0.95      0.95       974\n",
      "          9       0.95      0.94      0.94      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "[[ 971    1    0    0    0    1    5    1    1    0]\n",
      " [   0 1118    5    2    0    1    5    1    3    0]\n",
      " [   7    0  986    8    3    0    5   11   12    0]\n",
      " [   0    0   13  960    0   10    1   10   11    5]\n",
      " [   2    0    2    0  949    0    6    0    4   19]\n",
      " [   4    2    2   14    3  842   10    2    9    4]\n",
      " [   6    3    1    1    4    4  938    0    1    0]\n",
      " [   2    5   22    3    3    0    0  974    3   16]\n",
      " [   5    0    5    7    7    8    5    4  924    9]\n",
      " [   5    6    2   15   14    3    1    5    9  949]]\n",
      "Accuracy: 94.22\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97       980\n",
      "          1       0.97      0.99      0.98      1135\n",
      "          2       0.93      0.94      0.94      1032\n",
      "          3       0.91      0.93      0.92      1010\n",
      "          4       0.94      0.96      0.95       982\n",
      "          5       0.92      0.90      0.91       892\n",
      "          6       0.96      0.96      0.96       958\n",
      "          7       0.95      0.93      0.94      1028\n",
      "          8       0.93      0.89      0.91       974\n",
      "          9       0.95      0.91      0.93      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "[[ 965    0    2    2    1    3    5    1    1    0]\n",
      " [   0 1122    1    4    0    2    2    1    3    0]\n",
      " [   5    3  974    6    4    2    9   10   18    1]\n",
      " [   4    1   20  941    2   14    1    7   17    3]\n",
      " [   1    0    9    0  947    0    4    3    2   16]\n",
      " [  13    4    4   34    4  805    6    1   20    1]\n",
      " [   7    3    9    1    4   11  922    0    1    0]\n",
      " [   2   10   19    8   10    0    0  959    1   19]\n",
      " [   8    7    8   29    8   28    8    6  868    4]\n",
      " [   9    6    2    9   26    7    0   25    6  919]]\n"
     ]
    }
   ],
   "source": [
    "accuracy(bagging1,test[0],test[1])\n",
    "accuracy(bagging2,test[0],test[1])\n",
    "accuracy(bagging3,test[0],test[1])\n",
    "accuracy(bagging4,test[0],test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5o0mEqmk1RnM"
   },
   "source": [
    "## Boosting\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-Vq3ie-OfzS"
   },
   "outputs": [],
   "source": [
    "boosting4 = AdaBoostClassifier(base_estimator=clf1, n_estimators=5, algorithm='SAMME')\n",
    "start_time = time.time()\n",
    "boosting4.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "accuracy(boosting4,test[0],test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "c2XMAy6xwhxB",
    "outputId": "a7af2d58-0d66-4b65-85a3-f20630cb3947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 46.01915431022644 seconds ---\n",
      "--- 24.029749870300293 seconds ---\n"
     ]
    }
   ],
   "source": [
    "boosting1 = AdaBoostClassifier(base_estimator=clf_LR2, n_estimators=5)\n",
    "boosting3 = AdaBoostClassifier(base_estimator=clf_RBF1, n_estimators=5)\n",
    "start_time = time.time()\n",
    "boosting1.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "boosting3.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "98GJh1B7w-Ta",
    "outputId": "af528ab2-46e4-4a11-8c02-f831eaed7d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.31\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.88      0.91       980\n",
      "          1       0.92      0.94      0.93      1135\n",
      "          2       0.87      0.80      0.83      1032\n",
      "          3       0.81      0.85      0.83      1010\n",
      "          4       0.88      0.86      0.87       982\n",
      "          5       0.73      0.78      0.76       892\n",
      "          6       0.92      0.87      0.90       958\n",
      "          7       0.94      0.83      0.88      1028\n",
      "          8       0.77      0.83      0.80       974\n",
      "          9       0.78      0.88      0.83      1009\n",
      "\n",
      "avg / total       0.86      0.85      0.85     10000\n",
      "\n",
      "[[ 858    0   12    4    0   86   13    2    5    0]\n",
      " [   0 1070   11    6    0    7    4    0   37    0]\n",
      " [  13   18  822   38   21    8   17   17   71    7]\n",
      " [   4    3   16  857    0   58    4   13   42   13]\n",
      " [   1    8    6    0  841    5   11    1   14   95]\n",
      " [   9   13    5   94   16  697   14    3   21   20]\n",
      " [  16    7   28    1   22   38  838    0    8    0]\n",
      " [   1   28   26    3   14    4    0  850   19   83]\n",
      " [   5    9   12   45    9   35   11    5  811   32]\n",
      " [   7    9   12    9   36   14    0   13   22  887]]\n",
      "Accuracy: 95.59\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.98       980\n",
      "          1       0.98      0.99      0.98      1135\n",
      "          2       0.95      0.95      0.95      1032\n",
      "          3       0.94      0.95      0.95      1010\n",
      "          4       0.96      0.95      0.96       982\n",
      "          5       0.96      0.93      0.95       892\n",
      "          6       0.96      0.97      0.97       958\n",
      "          7       0.97      0.94      0.95      1028\n",
      "          8       0.93      0.94      0.93       974\n",
      "          9       0.94      0.94      0.94      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "[[ 964    0    1    1    1    3    5    2    3    0]\n",
      " [   0 1119    3    2    1    1    3    0    6    0]\n",
      " [   7    2  981    6    3    2    2    8   18    3]\n",
      " [   0    0   15  961    0   10    2    8   10    4]\n",
      " [   1    2    4    0  936    0    9    0    6   24]\n",
      " [   3    1    3   27    2  833   10    1    9    3]\n",
      " [   5    3    1    1    3    6  933    0    5    1]\n",
      " [   2    8   18    3    4    1    0  967    5   20]\n",
      " [   2    1    9   10    3   10    5    5  919   10]\n",
      " [   5    4    3   12   17    2    2    7   11  946]]\n"
     ]
    }
   ],
   "source": [
    "accuracy(boosting1,test[0],test[1])\n",
    "accuracy(boosting3,test[0],test[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "TVTp05cqxPsg",
    "outputId": "f903d4db-6e8f-4f7a-a773-0df21319a5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n",
      "Errors: 391  Correct :9609\n",
      "Accuracy: 96.09\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.98      0.99      0.98      1135\n",
      "          2       0.95      0.96      0.96      1032\n",
      "          3       0.94      0.96      0.95      1010\n",
      "          4       0.96      0.97      0.97       982\n",
      "          5       0.96      0.93      0.95       892\n",
      "          6       0.97      0.97      0.97       958\n",
      "          7       0.97      0.95      0.96      1028\n",
      "          8       0.95      0.94      0.94       974\n",
      "          9       0.96      0.94      0.95      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "[[ 970    0    1    1    0    2    4    1    1    0]\n",
      " [   0 1120    2    2    0    1    4    1    5    0]\n",
      " [   5    2  990    6    3    1    6    8   11    0]\n",
      " [   0    0   13  968    0    8    0   10    9    2]\n",
      " [   1    1    4    0  955    0    5    0    2   14]\n",
      " [   7    3    1   24    3  833    7    1   11    2]\n",
      " [   7    3    3    1    5    6  930    0    3    0]\n",
      " [   1    7   20    3    2    0    0  980    1   14]\n",
      " [   5    2    5   14    6   12    6    5  914    5]\n",
      " [   9    6    1   13   16    2    1    8    4  949]]\n"
     ]
    }
   ],
   "source": [
    "np.array(final).shape\n",
    "m = stats.mode(np.array(final))\n",
    "print(m[0][0])\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test[1], m[0][0]):\n",
    "    \n",
    "    if i == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(test[1], m[0][0]))\n",
    "print(confusion_matrix(test[1], m[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dp-yOzJx1WMw"
   },
   "source": [
    "## Stacking\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "UMQa6VMKtLuU",
    "outputId": "8407def7-5945-44bb-83e1-a82dbfc298bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2159.8025283813477 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2159.388486623764 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2150.5303370952606 seconds ---\n"
     ]
    }
   ],
   "source": [
    "clf_SVM = SVC(kernel='linear', probability=True)\n",
    "sclf1 = StackingClassifier(classifiers=[clf_SVM, clf_NN1, clf_RBF1], meta_classifier=clf_LR2)\n",
    "sclf2 = StackingClassifier(classifiers=[clf_SVM, clf_NN1, clf_RBF1], use_probas=True, average_probas=False, meta_classifier=clf_LR2)\n",
    "sclf3 = StackingClassifier(classifiers=[clf_SVM, clf_NN1, clf_RBF1], use_probas=True, average_probas=True, meta_classifier=clf_LR2)\n",
    "start_time = time.time()\n",
    "sclf1.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "sclf2.fit(train[0], train[1]) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "sclf3.fit(train[0], train[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1327
    },
    "colab_type": "code",
    "id": "PO4NlCeGxVVi",
    "outputId": "118ac04b-fdd9-47e6-f190-1813a0bcd598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.85\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.94      0.97       980\n",
      "          1       0.93      0.97      0.95      1135\n",
      "          2       0.89      0.91      0.90      1032\n",
      "          3       0.96      0.89      0.92      1010\n",
      "          4       0.88      0.91      0.90       982\n",
      "          5       0.90      0.86      0.88       892\n",
      "          6       0.91      0.94      0.93       958\n",
      "          7       0.80      0.93      0.86      1028\n",
      "          8       0.88      0.86      0.87       974\n",
      "          9       0.97      0.86      0.91      1009\n",
      "\n",
      "avg / total       0.91      0.91      0.91     10000\n",
      "\n",
      "[[ 925   23   20    2    2    2    0    0    6    0]\n",
      " [   0 1102   22    2    3    0    2    3    1    0]\n",
      " [   1   32  935    8   18    4   12   15    7    0]\n",
      " [   0    7   21  894   27   20   13   18   10    0]\n",
      " [   0    0    7    4  896   20   27   18    7    3]\n",
      " [   0    7    8   16   50  765   12   29    3    2]\n",
      " [   1    6    9    0   10    9  904    4   15    0]\n",
      " [   0    5   14    2    3    1    2  958   32   11]\n",
      " [   2    2   11    1    4   18    8   79  836   13]\n",
      " [   1    2    7    2    5   11   12   71   28  870]]\n",
      "Accuracy: 96.47\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.95      0.96      0.96      1032\n",
      "          3       0.95      0.97      0.96      1010\n",
      "          4       0.98      0.96      0.97       982\n",
      "          5       0.97      0.93      0.95       892\n",
      "          6       0.97      0.97      0.97       958\n",
      "          7       0.96      0.97      0.96      1028\n",
      "          8       0.94      0.95      0.94       974\n",
      "          9       0.96      0.95      0.95      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "[[ 971    0    1    0    0    2    1    2    3    0]\n",
      " [   0 1124    2    1    0    1    2    0    5    0]\n",
      " [   7    1  991    6    1    5    2    6   13    0]\n",
      " [   3    0    7  980    0    2    0    8    9    1]\n",
      " [   0    0    1    1  947    0    7    3    3   20]\n",
      " [   2    3   22   10    0  833    5    2   13    2]\n",
      " [   6    3    1    1    4    7  929    0    7    0]\n",
      " [   0    2    7    7    0    0    0  993    5   14]\n",
      " [   4    0    5    9    3    7   11    5  923    7]\n",
      " [   3    5    1   15   10    0    1   13    5  956]]\n",
      "Accuracy: 96.46000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.97      0.96      0.96      1032\n",
      "          3       0.94      0.97      0.96      1010\n",
      "          4       0.98      0.97      0.97       982\n",
      "          5       0.98      0.94      0.96       892\n",
      "          6       0.97      0.97      0.97       958\n",
      "          7       0.97      0.96      0.96      1028\n",
      "          8       0.93      0.95      0.94       974\n",
      "          9       0.96      0.95      0.96      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n",
      "[[ 968    0    1    0    0    2    3    2    4    0]\n",
      " [   0 1121    2    2    0    1    2    0    7    0]\n",
      " [   9    0  989    4    1    3    4    7   15    0]\n",
      " [   6    0    3  978    0    3    1    9    9    1]\n",
      " [   0    0    2    0  950    0    6    3    5   16]\n",
      " [   4    4    9   17    0  838    6    0   11    3]\n",
      " [   6    5    1    0    4    5  931    0    6    0]\n",
      " [   0    3    8    9    1    0    0  984    9   14]\n",
      " [   4    1    4   13    4    5    8    1  929    5]\n",
      " [   2    3    1   15   12    0    1   13    4  958]]\n"
     ]
    }
   ],
   "source": [
    "accuracy(sclf1,test[0],test[1])\n",
    "accuracy(sclf2,test[0],test[1])\n",
    "accuracy(sclf3,test[0],test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIFmf8dA1fdL"
   },
   "source": [
    "### Final Majority Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "C4l_pgV2xaZc",
    "outputId": "40f588ca-436d-4d15-a5a8-d1120312ff28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 10000)\n",
      "Errors: 349  Correct :9651\n",
      "Accuracy: 96.50999999999999\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.98      0.99      0.99      1135\n",
      "          2       0.96      0.97      0.96      1032\n",
      "          3       0.95      0.96      0.95      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.97      0.94      0.95       892\n",
      "          6       0.97      0.97      0.97       958\n",
      "          7       0.96      0.96      0.96      1028\n",
      "          8       0.95      0.95      0.95       974\n",
      "          9       0.97      0.94      0.95      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n",
      "[[ 971    0    1    0    0    2    2    1    3    0]\n",
      " [   0 1123    2    2    0    1    3    1    3    0]\n",
      " [   4    0  998    5    3    0    4    9    9    0]\n",
      " [   0    0   10  971    0    8    0   10   10    1]\n",
      " [   1    1    4    0  956    0    6    0    2   12]\n",
      " [   5    2    1   19    2  841    7    1   11    3]\n",
      " [   7    3    2    1    4    6  933    0    2    0]\n",
      " [   1    5   19    3    2    0    0  985    1   12]\n",
      " [   4    1    5   12    6   10    5    5  921    5]\n",
      " [   8    6    1   12   13    2    1    9    5  952]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = stats.mode(np.array(final))\n",
    "print(np.array(final).shape)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test[1], m[0][0]):\n",
    "    \n",
    "    if i == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(test[1], m[0][0]))\n",
    "print(confusion_matrix(test[1], m[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUIhTsZazluF"
   },
   "source": [
    "#**USPS DATA ** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sOBDC2lOz3dA"
   },
   "outputs": [],
   "source": [
    "usps_final=[]\n",
    "def usps_accuracy(clf,train,test):\n",
    "  right=0\n",
    "  wrong=0\n",
    "  y_pred = clf.predict(train)\n",
    "  for i,j in zip(test,y_pred):\n",
    "      if i==j:\n",
    "          right = right + 1\n",
    "      else:\n",
    "          wrong = wrong + 1\n",
    "  print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "  print(classification_report(test, y_pred))\n",
    "  print(confusion_matrix(test, y_pred))\n",
    "  usps_final.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "NhU3iIvE8AYX",
    "outputId": "f604d17f-153e-45d9-c5a1-e00b15990e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21499, 784)\n",
      "(21499,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import cv2\n",
    "image_list = []\n",
    "target=[]\n",
    "for i in range(0,10):\n",
    "    for filename in glob.glob('Numerals/'+str(i)+'/*.png'): \n",
    "        print(filename)\n",
    "        im=cv2.imread (filename)\n",
    "        im=cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        im=1-(im/255)\n",
    "        im=np.array(im)\n",
    "        im=cv2.resize(im, (28, 28)).flatten() \n",
    "        image_list.append(im)\n",
    "        target.append(i)\n",
    "count=0 \n",
    "for filename in glob.glob('Test/*.png'): \n",
    "    im=cv2.imread (filename)\n",
    "    im=cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    im=1-(im/255)\n",
    "    im=np.array(im)\n",
    "    im=cv2.resize(im, (28, 28)).flatten() \n",
    "    image_list.append(im)\n",
    "    target.append(9-(int(count/150)))\n",
    "    count=count+1\n",
    "image_list=np.array(image_list)\n",
    "print(image_list.shape) \n",
    "print(np.array(target).shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "HXFWnqX2xTVv",
    "outputId": "8ef28c72-faa7-4957-ce31-d4a06429acb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19999, 784)\n",
      "(19999,)\n"
     ]
    }
   ],
   "source": [
    "image_list=image_list[0:19999][:]\n",
    "target=target[0:19999]\n",
    "print(image_list.shape) \n",
    "print(np.array(target).shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcGrOga-38A"
   },
   "source": [
    "## Logistic Regression I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "9KjzALYq-ZxA",
    "outputId": "fa48af56-50de-4e5a-b5eb-70ea282f4adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 34.10170508525427\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.34      0.29      2000\n",
      "          1       0.34      0.20      0.25      2000\n",
      "          2       0.37      0.54      0.44      1999\n",
      "          3       0.40      0.57      0.47      2000\n",
      "          4       0.44      0.57      0.50      2000\n",
      "          5       0.44      0.45      0.44      2000\n",
      "          6       0.50      0.32      0.39      2000\n",
      "          7       0.20      0.15      0.17      2000\n",
      "          8       0.21      0.21      0.21      2000\n",
      "          9       0.16      0.07      0.09      2000\n",
      "\n",
      "avg / total       0.33      0.34      0.32     19999\n",
      "\n",
      "[[ 690    7  375   50  350   30   86   29   90  293]\n",
      " [ 241  396   55  252  294   57   50  330  309   16]\n",
      " [ 314   51 1077  113   89   44  106  102   83   20]\n",
      " [ 166    5  121 1136   44  191   49   90  131   67]\n",
      " [ 120  104   34   38 1136   79   25  125  229  110]\n",
      " [ 219   29  192  240   52  895  120   88  113   52]\n",
      " [ 505   14  374   88  134  144  640   16   65   20]\n",
      " [ 202  269  368  356   69   94   45  296  263   38]\n",
      " [ 269   48  181  188  192  432  139   39  424   88]\n",
      " [ 106  245  171  377  201   63   21  366  320  130]]\n"
     ]
    }
   ],
   "source": [
    "right=0\n",
    "wrong=0\n",
    "y_pred = make_prediction(image_list, W)\n",
    "for i,j in zip(target,y_pred):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(classification_report(target, y_pred))\n",
    "print(confusion_matrix(target, y_pred))\n",
    "usps_final.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6wwDWiI--zc"
   },
   "source": [
    "## Neural Network I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "etNgcvNB_BbY",
    "outputId": "c38fb4d6-bb54-4e92-b30a-81db92398b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39.42697134856743\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.25      0.34      2000\n",
      "          1       0.56      0.25      0.35      2000\n",
      "          2       0.51      0.61      0.56      1999\n",
      "          3       0.41      0.66      0.50      2000\n",
      "          4       0.58      0.45      0.51      2000\n",
      "          5       0.29      0.76      0.42      2000\n",
      "          6       0.76      0.40      0.53      2000\n",
      "          7       0.23      0.27      0.25      2000\n",
      "          8       0.28      0.14      0.19      2000\n",
      "          9       0.22      0.14      0.17      2000\n",
      "\n",
      "avg / total       0.44      0.39      0.38     19999\n",
      "\n",
      "[[ 496    2  138   61  181  350   47   57   96  572]\n",
      " [  32  508   48  176  146  316   17  652   55   50]\n",
      " [  64   30 1228  166   25  351   48   51   19   17]\n",
      " [  26    3  110 1327    2  444    2   41   31   14]\n",
      " [  16   33   50   25  898  265   25  347  125  216]\n",
      " [  41   17   97  167   13 1525   39   57   35    9]\n",
      " [ 140    5  366   89   90  458  808    8   16   20]\n",
      " [  25  180  153  554   23  396    4  531   93   41]\n",
      " [  77   21  107  295   42  982   69   70  280   57]\n",
      " [  14  114  109  405  120  179    7  504  264  284]]\n"
     ]
    }
   ],
   "source": [
    "right=0\n",
    "wrong=0\n",
    "predictedTestLabel=[]\n",
    "for i,j in zip(image_list,target):\n",
    "    y = model.predict(np.array(i).reshape(-1,784))\n",
    "    \n",
    "    predictedTestLabel.append(y.argmax())\n",
    "    \n",
    "    if j == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "#print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(target, predictedTestLabel))\n",
    "print(confusion_matrix(target, predictedTestLabel))\n",
    "usps_final.append(predictedTestLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2X9O2U_ASt_"
   },
   "source": [
    "## Logistic Regression II & III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "wqM2o_3WAYIB",
    "outputId": "ab596cc0-de39-4bbc-807a-da97a0cc4c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 34.991749587479376\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.31      0.37      2000\n",
      "          1       0.67      0.28      0.39      2000\n",
      "          2       0.44      0.60      0.51      1999\n",
      "          3       0.27      0.45      0.33      2000\n",
      "          4       0.39      0.44      0.42      2000\n",
      "          5       0.33      0.69      0.45      2000\n",
      "          6       0.59      0.30      0.40      2000\n",
      "          7       0.22      0.29      0.25      2000\n",
      "          8       0.17      0.09      0.12      2000\n",
      "          9       0.17      0.06      0.09      2000\n",
      "\n",
      "avg / total       0.37      0.35      0.33     19999\n",
      "\n",
      "[[ 627    2  151  137  217  264   70  187   80  265]\n",
      " [  62  550   41  209  501  135   25  355   96   26]\n",
      " [  93   50 1198  146   77  211   73   81   41   29]\n",
      " [  43   10  301  897   19  581    9   58   51   31]\n",
      " [  46   42   47   77  880  146   21  476  179   86]\n",
      " [  74   15   98  199   48 1375   70   75   32   14]\n",
      " [  90    8  513  117  102  531  601   14   14   10]\n",
      " [ 100   83  191  545  119  214   17  573  114   44]\n",
      " [ 192   20  107  508  135  580  116  111  181   50]\n",
      " [  36   40   95  526  133  109   11  642  292  116]]\n",
      "Accuracy: 34.51672583629182\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.25      0.32      2000\n",
      "          1       0.76      0.26      0.38      2000\n",
      "          2       0.45      0.60      0.52      1999\n",
      "          3       0.28      0.44      0.34      2000\n",
      "          4       0.45      0.38      0.41      2000\n",
      "          5       0.31      0.66      0.43      2000\n",
      "          6       0.67      0.32      0.43      2000\n",
      "          7       0.20      0.35      0.26      2000\n",
      "          8       0.17      0.10      0.13      2000\n",
      "          9       0.19      0.10      0.13      2000\n",
      "\n",
      "avg / total       0.39      0.35      0.33     19999\n",
      "\n",
      "[[ 504    1   86  104  148  276   46  301   96  438]\n",
      " [  47  513   30  155  383  159   22  544  112   35]\n",
      " [  77   44 1196  145   53  294   82   46   37   25]\n",
      " [  32    7  281  876   12  645    3   83   42   19]\n",
      " [  30    8   53   40  768  105   17  666  166  147]\n",
      " [  82   10   91  212   44 1322   46   99   70   24]\n",
      " [ 108    9  610   69   81  437  637   24   11   14]\n",
      " [  79   47   73  573   53  236    9  696  176   58]\n",
      " [ 177   12  142  476   80  643   81  150  201   38]\n",
      " [  15   20   72  453   90   80    2  802  276  190]]\n"
     ]
    }
   ],
   "source": [
    "usps_accuracy(clf_LR1,image_list,target)\n",
    "usps_accuracy(clf_LR2,image_list,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_4VQpET_msu"
   },
   "source": [
    "## Neural Network II & III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "X1u6VrLGBCYi",
    "outputId": "18e290e5-335a-4a1a-bcb4-c44a511aba3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.511875593779685\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.21      0.23      2000\n",
      "          1       0.39      0.17      0.24      2000\n",
      "          2       0.40      0.49      0.44      1999\n",
      "          3       0.38      0.47      0.42      2000\n",
      "          4       0.74      0.37      0.49      2000\n",
      "          5       0.70      0.68      0.69      2000\n",
      "          6       0.58      0.47      0.52      2000\n",
      "          7       0.20      0.24      0.22      2000\n",
      "          8       0.23      0.42      0.29      2000\n",
      "          9       0.24      0.23      0.23      2000\n",
      "\n",
      "avg / total       0.41      0.38      0.38     19999\n",
      "\n",
      "[[ 417    8  126  309   62   21   68  341  293  355]\n",
      " [  23  339   64  150   15   41   56  430  438  444]\n",
      " [ 245   53  989   95    3  138   24   35  403   14]\n",
      " [ 384   14  250  935    1   78    4  101  227    6]\n",
      " [   9   60   13   69  740    4  290  193  163  459]\n",
      " [  72   85  308   37    3 1351   12   17  107    8]\n",
      " [  27  168  121   39   58  102  946   33  462   44]\n",
      " [ 258   14  256  440    2   16   10  479  403  122]\n",
      " [ 129   75  306  157    8  156  158  119  843   49]\n",
      " [  35   54   51  224  102   11   64  607  389  463]]\n",
      "Accuracy: 31.816590829541475\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.10      0.16      2000\n",
      "          1       0.25      0.20      0.22      2000\n",
      "          2       0.34      0.63      0.44      1999\n",
      "          3       0.70      0.43      0.53      2000\n",
      "          4       0.55      0.51      0.53      2000\n",
      "          5       0.40      0.18      0.25      2000\n",
      "          6       0.41      0.29      0.34      2000\n",
      "          7       0.14      0.32      0.20      2000\n",
      "          8       0.19      0.30      0.23      2000\n",
      "          9       0.40      0.22      0.29      2000\n",
      "\n",
      "avg / total       0.38      0.32      0.32     19999\n",
      "\n",
      "[[ 193  226  301   30  197   23  104  576  141  209]\n",
      " [   2  399  121   22  334    1  283  643   53  142]\n",
      " [  26  144 1268    2   11  117   79  142  209    1]\n",
      " [   2    6   87  853    1  143   12  217  622   57]\n",
      " [   0   19   33    4 1020    0  103  645   65  111]\n",
      " [  24   78  508   63   16  361   58  281  605    6]\n",
      " [ 164  512  470    0   60   24  580  110   77    3]\n",
      " [   4   59  385   67   31   91   60  637  558  108]\n",
      " [  49  104  449  127   43  131   74  393  610   20]\n",
      " [   1   19  114   49  129   15   60  862  309  442]]\n"
     ]
    }
   ],
   "source": [
    "usps_accuracy(clf_NN1,image_list,target)\n",
    "usps_accuracy(clf_NN2,image_list,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TatsnTnrviEH"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xl0_2GNwvk15"
   },
   "outputs": [],
   "source": [
    "(X_test, y_test)=(np.array(image_list),one_hot_encode(target, num_classes))\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "score = CNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred=CNN_model.predict(X_test)\n",
    "y_pred=np.array([np.argmax(t) for t in y_pred])\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(target,y_pred):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(classification_report(target, y_pred))\n",
    "print(confusion_matrix(target, y_pred))\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivyfQA-sBPwc"
   },
   "source": [
    "## RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1327
    },
    "colab_type": "code",
    "id": "9_7Pn0DWBR4Y",
    "outputId": "89bd39ff-6103-4448-f24c-b54db5bb5d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.18185909295465\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.31      0.33      2000\n",
      "          1       0.40      0.37      0.39      2000\n",
      "          2       0.33      0.58      0.42      1999\n",
      "          3       0.39      0.58      0.47      2000\n",
      "          4       0.45      0.52      0.49      2000\n",
      "          5       0.40      0.62      0.49      2000\n",
      "          6       0.63      0.32      0.42      2000\n",
      "          7       0.22      0.26      0.24      2000\n",
      "          8       0.42      0.09      0.15      2000\n",
      "          9       0.22      0.07      0.11      2000\n",
      "\n",
      "avg / total       0.38      0.37      0.35     19999\n",
      "\n",
      "[[ 620   31  330   77  372  126   98   62   18  266]\n",
      " [  32  749  140  160   80   87   34  705    5    8]\n",
      " [ 183   71 1151  132   79  155   31  161   16   20]\n",
      " [ 102   38  206 1153   57  281   10   94   24   35]\n",
      " [  40  174  137   98 1045   96   31  278   37   64]\n",
      " [ 128   71  133  234   50 1232   40   80   12   20]\n",
      " [ 409   64  288   86  184  241  636   53   20   19]\n",
      " [  67  339  494  333   43  133   23  527   17   24]\n",
      " [ 140   76  269  297  186  627   89   76  182   58]\n",
      " [  54  246  376  384  208   93   20  380   98  141]]\n",
      "Accuracy: 44.84224211210561\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.31      0.38      2000\n",
      "          1       0.53      0.36      0.43      2000\n",
      "          2       0.43      0.71      0.53      1999\n",
      "          3       0.55      0.71      0.62      2000\n",
      "          4       0.53      0.60      0.57      2000\n",
      "          5       0.40      0.80      0.54      2000\n",
      "          6       0.77      0.47      0.58      2000\n",
      "          7       0.22      0.31      0.25      2000\n",
      "          8       0.49      0.13      0.21      2000\n",
      "          9       0.24      0.08      0.12      2000\n",
      "\n",
      "avg / total       0.46      0.45      0.42     19999\n",
      "\n",
      "[[ 629    9  288   53  424  109   55   40    0  393]\n",
      " [  14  716   59  140   24  114   42  888    2    1]\n",
      " [  83   22 1422   72   52  137   14  187    7    3]\n",
      " [  37    5  111 1422   41  262    3   99    6   14]\n",
      " [   8  148   55   26 1210  133   12  315   55   38]\n",
      " [  96   15   86   70   24 1598   20   73   10    8]\n",
      " [ 304   29  207   28  115  306  937   45   14   15]\n",
      " [  33  223  613  220   25  212   50  611    5    8]\n",
      " [  77   19  181  207  113  959   75   70  263   36]\n",
      " [  10  166  320  326  250  116   14  468  170  160]]\n",
      "Accuracy: 45.34226711335567\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.34      0.40      2000\n",
      "          1       0.50      0.34      0.40      2000\n",
      "          2       0.45      0.71      0.55      1999\n",
      "          3       0.54      0.71      0.61      2000\n",
      "          4       0.53      0.60      0.57      2000\n",
      "          5       0.41      0.81      0.55      2000\n",
      "          6       0.80      0.50      0.61      2000\n",
      "          7       0.21      0.30      0.25      2000\n",
      "          8       0.54      0.14      0.22      2000\n",
      "          9       0.28      0.09      0.14      2000\n",
      "\n",
      "avg / total       0.47      0.45      0.43     19999\n",
      "\n",
      "[[ 671    6  258   67  440   90   56   40    4  368]\n",
      " [   9  676   54  125   38  123   36  932    6    1]\n",
      " [ 100   21 1413   63   53  143   15  182    6    3]\n",
      " [  42    3   78 1418   46  261    2  126    5   19]\n",
      " [   7  154   46   26 1209  142   15  312   57   32]\n",
      " [ 106   16   85   70   11 1625   12   61    5    9]\n",
      " [ 311   28  216   30  101  260  991   44   10    9]\n",
      " [  48  230  528  275   30  233   37  604    4   11]\n",
      " [  66   22  182  226   97  965   68   61  274   39]\n",
      " [  23  186  310  344  235   87   14  473  141  187]]\n"
     ]
    }
   ],
   "source": [
    "usps_accuracy(clf_RBF1,image_list,target)\n",
    "usps_accuracy(clf_RBF2,image_list,target)\n",
    "usps_accuracy(clf_RBF3,image_list,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrzwLf3fBoaR"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "QVdULPjSBqRD",
    "outputId": "bfcb2f34-4184-460d-8665-9beaa57debed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.97169858492924\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.26      0.33      2000\n",
      "          1       0.58      0.24      0.34      2000\n",
      "          2       0.34      0.61      0.44      1999\n",
      "          3       0.28      0.47      0.35      2000\n",
      "          4       0.43      0.45      0.44      2000\n",
      "          5       0.30      0.64      0.41      2000\n",
      "          6       0.70      0.29      0.41      2000\n",
      "          7       0.23      0.27      0.25      2000\n",
      "          8       0.24      0.10      0.14      2000\n",
      "          9       0.22      0.09      0.12      2000\n",
      "\n",
      "avg / total       0.38      0.34      0.32     19999\n",
      "\n",
      "[[ 513    0  302   55  266  311   51  117    6  379]\n",
      " [  50  474  150  310  346  172   14  408   48   28]\n",
      " [ 179   81 1216  123   55  209   42   51   28   15]\n",
      " [  56   59  303  932   12  519    7   45   46   21]\n",
      " [  21   24  133   72  900  192    9  464   92   93]\n",
      " [  48   11  165  242   74 1280   30   50   79   21]\n",
      " [ 158   18  728   45  131  323  575   14    3    5]\n",
      " [  16   72  172  699   43  322   12  543   94   27]\n",
      " [  90   26  260  385  102  803   76   53  191   14]\n",
      " [  10   46  151  511  156  107    7  630  212  170]]\n",
      "Accuracy: 40.857042852142605\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.30      0.36      2000\n",
      "          1       0.51      0.31      0.38      2000\n",
      "          2       0.40      0.67      0.50      1999\n",
      "          3       0.53      0.57      0.55      2000\n",
      "          4       0.50      0.62      0.55      2000\n",
      "          5       0.31      0.74      0.43      2000\n",
      "          6       0.67      0.40      0.50      2000\n",
      "          7       0.24      0.23      0.24      2000\n",
      "          8       0.39      0.14      0.21      2000\n",
      "          9       0.26      0.11      0.16      2000\n",
      "\n",
      "avg / total       0.43      0.41      0.39     19999\n",
      "\n",
      "[[ 594    3  351   18  295  207   72   32    5  423]\n",
      " [  63  611   76  135  332  217   42  493   15   16]\n",
      " [ 132   27 1341   66   58  198   69   71   25   12]\n",
      " [  68    5  151 1149   13  492    6   65   27   24]\n",
      " [  16   73   59   11 1231  231   21  196   61  101]\n",
      " [  84   20  136  110   25 1484   50   52   27   12]\n",
      " [ 192    9  417   21  130  411  797    4    9   10]\n",
      " [  42  236  444  253   54  420   17  461   48   25]\n",
      " [  70   25  182  182   90 1014   94   36  279   28]\n",
      " [  24  197  192  236  232  158   13  505  219  224]]\n"
     ]
    }
   ],
   "source": [
    "usps_accuracy(clf1,image_list,target)\n",
    "usps_accuracy(clf3,image_list,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2X5u69hOBue8"
   },
   "outputs": [],
   "source": [
    "usps_accuracy(clf2,image_list,target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Agf2a7rbCijv"
   },
   "source": [
    "### Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "IEhA-nW8CKm2",
    "outputId": "1cdcb159-6c55-4ca6-bffe-d405d78bf7eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 19999)\n",
      "Errors: 11408  Correct :8591\n",
      "Accuracy: 42.95714785739287\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.33      0.39      2000\n",
      "          1       0.55      0.32      0.40      2000\n",
      "          2       0.45      0.72      0.55      1999\n",
      "          3       0.45      0.66      0.53      2000\n",
      "          4       0.55      0.60      0.57      2000\n",
      "          5       0.37      0.76      0.50      2000\n",
      "          6       0.76      0.41      0.53      2000\n",
      "          7       0.23      0.28      0.25      2000\n",
      "          8       0.33      0.14      0.19      2000\n",
      "          9       0.25      0.09      0.13      2000\n",
      "\n",
      "avg / total       0.44      0.43      0.41     19999\n",
      "\n",
      "[[ 660    1  291   51  322  147   43   78   18  389]\n",
      " [  37  638   50  179  230  155   32  630   44    5]\n",
      " [ 115   31 1439   79   44  153   39   72   17   10]\n",
      " [  54    2  153 1311   10  377    3   57   25    8]\n",
      " [  21   93   43   23 1207  134   13  295  104   67]\n",
      " [  82   15  119  133   25 1511   30   60   21    4]\n",
      " [ 260   13  439   35  106  311  818    8    3    7]\n",
      " [  56  179  343  435   25  288   16  564   78   16]\n",
      " [ 120   21  172  289   81  881   72   74  272   18]\n",
      " [  17  163  170  380  158  103    8  590  240  171]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = stats.mode(np.array(usps_final))\n",
    "print(np.array(usps_final).shape)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(target, m[0][0]):\n",
    "    \n",
    "    if i == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(target, m[0][0]))\n",
    "print(confusion_matrix(target, m[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfjB-Z0MCmf8"
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "ZuMextj6CwyF",
    "outputId": "3561ecb1-860f-4721-b389-a12632b9a28c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 32.06160308015401\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.11      0.19      2000\n",
      "          1       0.36      0.27      0.31      2000\n",
      "          2       0.34      0.51      0.41      1999\n",
      "          3       0.47      0.44      0.45      2000\n",
      "          4       0.46      0.54      0.50      2000\n",
      "          5       0.22      0.74      0.34      2000\n",
      "          6       0.62      0.21      0.32      2000\n",
      "          7       0.20      0.19      0.20      2000\n",
      "          8       0.28      0.09      0.14      2000\n",
      "          9       0.22      0.10      0.14      2000\n",
      "\n",
      "avg / total       0.38      0.32      0.30     19999\n",
      "\n",
      "[[ 217   14  503   32  403  404   80   53    6  288]\n",
      " [  16  537   73  166  204  367    8  566   40   23]\n",
      " [  29   78 1021   86   89  467   48  121   44   16]\n",
      " [  19    7  114  878   29  726   20   89   46   72]\n",
      " [   6  114   56   16 1087  357    7  160   67  130]\n",
      " [  17   47  142  137   21 1473   38   78   12   35]\n",
      " [  28   16  449   36  166  820  428   29   14   14]\n",
      " [   1  374  300  141   77  617    6  384   57   43]\n",
      " [   6   46  155  111  126 1205   51   40  185   75]\n",
      " [   6  265  181  281  159  315    6  394  191  202]]\n",
      "Accuracy: 39.66698334916746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.26      0.33      2000\n",
      "          1       0.44      0.30      0.36      2000\n",
      "          2       0.40      0.63      0.49      1999\n",
      "          3       0.49      0.58      0.53      2000\n",
      "          4       0.40      0.57      0.47      2000\n",
      "          5       0.37      0.67      0.48      2000\n",
      "          6       0.70      0.41      0.52      2000\n",
      "          7       0.21      0.24      0.22      2000\n",
      "          8       0.32      0.20      0.25      2000\n",
      "          9       0.26      0.10      0.14      2000\n",
      "\n",
      "avg / total       0.40      0.40      0.38     19999\n",
      "\n",
      "[[ 519    7  326   50  447  126   93   58   53  321]\n",
      " [  22  606  153  111  321  120   35  568   47   17]\n",
      " [  95   37 1262   92   71  159   39  155   73   16]\n",
      " [  50    8  154 1158   39  365   18   85   79   44]\n",
      " [  11  128   77   35 1150  144   25  231  130   69]\n",
      " [  87   30  123  151   37 1345   22  100   81   24]\n",
      " [ 242   19  260   70  174  297  814   38   67   19]\n",
      " [  74  284  419  231  131  211   32  472  125   21]\n",
      " [  64   28  202  250  144  716   68   73  407   48]\n",
      " [  24  224  186  212  354  119    9  446  226  200]]\n"
     ]
    }
   ],
   "source": [
    "usps_accuracy(boosting1,image_list,target)\n",
    "usps_accuracy(boosting3,image_list,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdShvI4_C3XH"
   },
   "outputs": [],
   "source": [
    "usps_accuracy(boosting4,image_list,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "Ef4nHq9_DFBF",
    "outputId": "2b5992cf-38e4-4827-9b67-a8be9289faaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 19999)\n",
      "Errors: 11448  Correct :8551\n",
      "Accuracy: 42.75713785689285\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.32      0.38      2000\n",
      "          1       0.52      0.32      0.40      2000\n",
      "          2       0.45      0.71      0.55      1999\n",
      "          3       0.47      0.65      0.55      2000\n",
      "          4       0.54      0.61      0.57      2000\n",
      "          5       0.35      0.76      0.48      2000\n",
      "          6       0.77      0.41      0.53      2000\n",
      "          7       0.23      0.27      0.25      2000\n",
      "          8       0.34      0.14      0.19      2000\n",
      "          9       0.24      0.08      0.13      2000\n",
      "\n",
      "avg / total       0.44      0.43      0.40     19999\n",
      "\n",
      "[[ 633    1  316   46  354  152   48   62   15  373]\n",
      " [  31  648   46  176  189  180   27  664   35    4]\n",
      " [ 115   32 1418   72   51  167   36   80   19    9]\n",
      " [  51    3  135 1297   15  394    3   64   27   11]\n",
      " [  13   99   40   21 1225  154   13  263   98   74]\n",
      " [  72   18  127  126   21 1527   27   59   18    5]\n",
      " [ 228   13  402   32  108  376  816    9    6   10]\n",
      " [  47  218  352  364   35  327   14  546   79   18]\n",
      " [  87   24  164  250   92  952   66   66  272   27]\n",
      " [  16  189  173  357  171  119    9  559  238  169]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = stats.mode(np.array(usps_final))\n",
    "print(np.array(usps_final).shape)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(target, m[0][0]):\n",
    "    \n",
    "    if i == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(target, m[0][0]))\n",
    "print(confusion_matrix(target, m[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6H2IChYDL8h"
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1764
    },
    "colab_type": "code",
    "id": "-3NmbcpZDMQD",
    "outputId": "44f4d804-a29a-4c7a-86ed-5d2968633387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 34.69173458672934\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.25      0.32      2000\n",
      "          1       0.75      0.26      0.39      2000\n",
      "          2       0.42      0.60      0.49      1999\n",
      "          3       0.30      0.44      0.35      2000\n",
      "          4       0.45      0.41      0.43      2000\n",
      "          5       0.31      0.66      0.43      2000\n",
      "          6       0.68      0.31      0.43      2000\n",
      "          7       0.20      0.35      0.26      2000\n",
      "          8       0.20      0.11      0.14      2000\n",
      "          9       0.18      0.08      0.11      2000\n",
      "\n",
      "avg / total       0.39      0.35      0.33     19999\n",
      "\n",
      "[[ 496    1  141   98  174  275   42  299   60  414]\n",
      " [  49  523   35  154  386  171   18  529  106   29]\n",
      " [ 100   48 1193  128   55  291   61   57   38   28]\n",
      " [  38    8  290  877    9  626    6   76   53   17]\n",
      " [  28   12   60   32  818  104   18  654  154  120]\n",
      " [  78    7  117  196   45 1321   46  103   62   25]\n",
      " [ 128    7  598   75   85  433  624   26   11   13]\n",
      " [  65   57  113  571   61  227    9  706  146   45]\n",
      " [ 151   13  175  436   84  659   83  151  216   32]\n",
      " [  14   25  101  396   98  100    6  845  251  164]]\n",
      "Accuracy: 46.23231161558078\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.28      0.36      2000\n",
      "          1       0.52      0.27      0.36      2000\n",
      "          2       0.49      0.70      0.58      1999\n",
      "          3       0.49      0.72      0.58      2000\n",
      "          4       0.53      0.51      0.52      2000\n",
      "          5       0.78      0.64      0.71      2000\n",
      "          6       0.85      0.51      0.64      2000\n",
      "          7       0.35      0.27      0.31      2000\n",
      "          8       0.28      0.48      0.35      2000\n",
      "          9       0.20      0.23      0.21      2000\n",
      "\n",
      "avg / total       0.50      0.46      0.46     19999\n",
      "\n",
      "[[ 563   17  102   56  197   27   48   27  367  596]\n",
      " [  28  541  303  123  204   25   18  233  232  293]\n",
      " [  33   56 1405  176   59   19    1   40  157   53]\n",
      " [  21   13  174 1446    3   64    2   33  185   59]\n",
      " [  28   69   32   23 1026   22   26  140  284  350]\n",
      " [  48   22   34   94   20 1282   23   17  323  137]\n",
      " [ 169  117   61   81  222   68 1023    7  199   53]\n",
      " [  19   85  528  433   35    6    2  546  241  105]\n",
      " [ 188   39   80  274  107  110   51   41  963  147]\n",
      " [  16   77  156  260   77   13    6  463  481  451]]\n",
      "Accuracy: 42.22711135556778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.32      0.38      2000\n",
      "          1       0.46      0.33      0.38      2000\n",
      "          2       0.41      0.68      0.51      1999\n",
      "          3       0.49      0.66      0.56      2000\n",
      "          4       0.52      0.58      0.55      2000\n",
      "          5       0.39      0.74      0.51      2000\n",
      "          6       0.71      0.44      0.55      2000\n",
      "          7       0.20      0.29      0.24      2000\n",
      "          8       0.47      0.12      0.19      2000\n",
      "          9       0.25      0.07      0.11      2000\n",
      "\n",
      "avg / total       0.44      0.42      0.40     19999\n",
      "\n",
      "[[ 633   16  292   58  413  111   89   70   12  306]\n",
      " [  18  655   77  140   45  118   41  898    6    2]\n",
      " [  89   30 1365   80   45  152   33  187   14    4]\n",
      " [  42   10  122 1317   36  318    3  126    8   18]\n",
      " [  11  158   54   30 1159  160   24  309   52   43]\n",
      " [  93   22  116  137   17 1472   24   99    9   11]\n",
      " [ 314   43  262   44  113  258  888   54   12   12]\n",
      " [  34  258  575  286   33  179   39  576   11    9]\n",
      " [  75   35  198  218  119  911   98   77  237   32]\n",
      " [  23  209  289  353  251  105   13  474  140  143]]\n",
      "Accuracy: 35.871793589679484\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.29      0.34      2000\n",
      "          1       0.55      0.26      0.36      2000\n",
      "          2       0.36      0.64      0.46      1999\n",
      "          3       0.31      0.50      0.38      2000\n",
      "          4       0.47      0.51      0.49      2000\n",
      "          5       0.32      0.66      0.43      2000\n",
      "          6       0.71      0.28      0.40      2000\n",
      "          7       0.23      0.27      0.25      2000\n",
      "          8       0.28      0.10      0.15      2000\n",
      "          9       0.20      0.07      0.11      2000\n",
      "\n",
      "avg / total       0.39      0.36      0.34     19999\n",
      "\n",
      "[[ 574    2  290   54  258  243   42  131    7  399]\n",
      " [  57  527  128  268  318  178   24  443   43   14]\n",
      " [ 189   63 1278  104   57  185   45   40   21   17]\n",
      " [  84   27  239 1007   10  529    9   43   29   23]\n",
      " [  24   47  102   54 1029  180   16  396   68   84]\n",
      " [  69   22  149  238   51 1325   26   51   53   16]\n",
      " [ 197   15  741   55  135  288  559    9    0    1]\n",
      " [  25  123  188  662   43  324    7  531   75   22]\n",
      " [ 108   24  264  371  115  805   54   47  195   17]\n",
      " [  14  114  148  472  169  116    8  615  195  149]]\n"
     ]
    }
   ],
   "source": [
    "usps_accuracy(bagging1,image_list,target)\n",
    "usps_accuracy(bagging2,image_list,target)\n",
    "usps_accuracy(bagging3,image_list,target)\n",
    "usps_accuracy(bagging4,image_list,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "wAy7_bJqDXOL",
    "outputId": "cbc292fe-711d-4e1d-8cd7-25413371309f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 19999)\n",
      "Errors: 11390  Correct :8609\n",
      "Accuracy: 43.04715235761788\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.32      0.39      2000\n",
      "          1       0.55      0.32      0.40      2000\n",
      "          2       0.45      0.72      0.55      1999\n",
      "          3       0.45      0.65      0.53      2000\n",
      "          4       0.55      0.61      0.58      2000\n",
      "          5       0.36      0.77      0.49      2000\n",
      "          6       0.79      0.41      0.54      2000\n",
      "          7       0.23      0.28      0.25      2000\n",
      "          8       0.35      0.14      0.20      2000\n",
      "          9       0.24      0.09      0.13      2000\n",
      "\n",
      "avg / total       0.45      0.43      0.41     19999\n",
      "\n",
      "[[ 634    1  301   48  332  156   40   71   14  403]\n",
      " [  35  640   48  181  197  179   27  657   32    4]\n",
      " [  99   33 1438   75   48  174   32   74   15   11]\n",
      " [  46    3  162 1297   10  388    2   58   24   10]\n",
      " [  15   92   41   18 1226  151   10  281   97   69]\n",
      " [  69   15  113  133   24 1535   22   61   23    5]\n",
      " [ 219   13  431   35  109  353  822    9    4    5]\n",
      " [  41  193  329  418   28  322    9  561   82   17]\n",
      " [  91   16  159  294   93  902   68   70  285   22]\n",
      " [  13  164  165  375  156  115    5  595  241  171]]\n"
     ]
    }
   ],
   "source": [
    "m = stats.mode(np.array(usps_final))\n",
    "print(np.array(usps_final).shape)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(target, m[0][0]):\n",
    "    \n",
    "    if i == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(target, m[0][0]))\n",
    "print(confusion_matrix(target, m[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsQ4N9A0DASe"
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1327
    },
    "colab_type": "code",
    "id": "NGzSQRHDDeRu",
    "outputId": "160a7bd7-b9c6-4fb0-f33b-d7d35b9fdc89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 26.696334816740837\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.12      0.21      2000\n",
      "          1       0.23      0.15      0.18      2000\n",
      "          2       0.25      0.54      0.34      1999\n",
      "          3       0.44      0.38      0.41      2000\n",
      "          4       0.22      0.29      0.25      2000\n",
      "          5       0.30      0.46      0.36      2000\n",
      "          6       0.40      0.28      0.33      2000\n",
      "          7       0.17      0.24      0.20      2000\n",
      "          8       0.16      0.16      0.16      2000\n",
      "          9       0.30      0.04      0.07      2000\n",
      "\n",
      "avg / total       0.33      0.27      0.25     19999\n",
      "\n",
      "[[ 235  230  421   63  117  222   75  359  161  117]\n",
      " [   0  307  492   75   80  195   64  283  499    5]\n",
      " [  13  194 1082  199   96  119  141   62   92    1]\n",
      " [   4  114  243  750  431  177  142   81   54    4]\n",
      " [   0   33  196   49  581  343   71  484  231   12]\n",
      " [   4   85  192  124  380  928  116  112   42   17]\n",
      " [  11   98  544   63  192  285  559   33  206    9]\n",
      " [   5  148  504  134  295  128   88  486  209    3]\n",
      " [   7   79  294  152  350  496   92  181  329   20]\n",
      " [   1   37  372   80  159  225   58  697  289   82]]\n",
      "Accuracy: 42.95214760738037\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.26      0.32      2000\n",
      "          1       0.56      0.24      0.34      2000\n",
      "          2       0.44      0.62      0.52      1999\n",
      "          3       0.39      0.65      0.48      2000\n",
      "          4       0.74      0.41      0.53      2000\n",
      "          5       0.63      0.73      0.67      2000\n",
      "          6       0.65      0.50      0.56      2000\n",
      "          7       0.22      0.29      0.25      2000\n",
      "          8       0.30      0.35      0.32      2000\n",
      "          9       0.27      0.23      0.25      2000\n",
      "\n",
      "avg / total       0.46      0.43      0.43     19999\n",
      "\n",
      "[[ 528    4  144  307   89   34   67  266  132  429]\n",
      " [  20  478   55  205   13   55   29  717  200  228]\n",
      " [ 145   38 1246  158    7   99   26   80  189   11]\n",
      " [ 133   11  239 1306    1  139    4   52  103   12]\n",
      " [  10   56   21   79  829   20  215  244  131  395]\n",
      " [  62   37  262   66    5 1451   17   28   65    7]\n",
      " [ 114  115  196   69   53  157  995   27  234   40]\n",
      " [ 113   33  314  580    3   51   10  589  237   70]\n",
      " [ 114   31  273  295   11  288  140   89  698   61]\n",
      " [  15   47   68  324  104   15   38  573  346  470]]\n",
      "Accuracy: 41.257062853142656\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.24      0.31      2000\n",
      "          1       0.58      0.23      0.33      2000\n",
      "          2       0.44      0.64      0.52      1999\n",
      "          3       0.36      0.60      0.45      2000\n",
      "          4       0.72      0.44      0.55      2000\n",
      "          5       0.49      0.72      0.59      2000\n",
      "          6       0.66      0.45      0.54      2000\n",
      "          7       0.24      0.29      0.26      2000\n",
      "          8       0.26      0.30      0.28      2000\n",
      "          9       0.25      0.20      0.22      2000\n",
      "\n",
      "avg / total       0.44      0.41      0.41     19999\n",
      "\n",
      "[[ 483    3  180  241  105  108   57  209  143  471]\n",
      " [  13  455   45  219   36   95   30  552  288  267]\n",
      " [ 149   35 1277  124   10  125   22   56  190   11]\n",
      " [ 121   11  213 1205    0  295    5   43  101    6]\n",
      " [   7   63   35   64  888   52  170  269  137  315]\n",
      " [  57   39  215  117    7 1447   15   24   71    8]\n",
      " [  62   86  371   44   66  200  908   13  228   22]\n",
      " [ 110   27  250  619    7  108   10  580  237   52]\n",
      " [  74   29  225  325   20  476  130   80  606   35]\n",
      " [  15   39   68  402  100   36   28  565  345  402]]\n"
     ]
    }
   ],
   "source": [
    "usps_accuracy(sclf1,image_list,target)\n",
    "usps_accuracy(sclf2,image_list,target)\n",
    "usps_accuracy(sclf3,image_list,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-8kxImIDsEm"
   },
   "source": [
    "### Final Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "uVoVOxeRDqwL",
    "outputId": "6844f994-a082-448c-cc7a-32b2522e559f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 19999)\n",
      "Errors: 11260  Correct :8739\n",
      "Accuracy: 43.69718485924296\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.31      0.39      2000\n",
      "          1       0.56      0.31      0.40      2000\n",
      "          2       0.46      0.73      0.56      1999\n",
      "          3       0.46      0.66      0.54      2000\n",
      "          4       0.57      0.60      0.59      2000\n",
      "          5       0.37      0.77      0.50      2000\n",
      "          6       0.81      0.43      0.56      2000\n",
      "          7       0.23      0.30      0.26      2000\n",
      "          8       0.34      0.15      0.21      2000\n",
      "          9       0.25      0.10      0.14      2000\n",
      "\n",
      "avg / total       0.46      0.44      0.42     19999\n",
      "\n",
      "[[ 628    1  289   48  306  154   40   97   22  415]\n",
      " [  28  622   53  179  169  172   25  703   42    7]\n",
      " [ 104   30 1453   73   46  164   29   75   17    8]\n",
      " [  45    4  154 1326    8  372    2   56   26    7]\n",
      " [  11   91   42   19 1210  136   11  305  101   74]\n",
      " [  66   14  121  133   18 1540   20   58   25    5]\n",
      " [ 193   18  429   34  104  335  870    8    4    5]\n",
      " [  44  165  318  454   24  288   10  591   91   15]\n",
      " [  93   15  167  281   83  893   67   69  309   23]\n",
      " [   9  142  153  366  146  101    6  625  262  190]]\n"
     ]
    }
   ],
   "source": [
    "m = stats.mode(np.array(usps_final))\n",
    "print(np.array(usps_final).shape)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(target, m[0][0]):\n",
    "    \n",
    "    if i == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100))\n",
    "print(classification_report(target, m[0][0]))\n",
    "print(confusion_matrix(target, m[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9h80xTaUbcs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
